{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clean uninstall just in case\n# !pip uninstall -y torch-scatter torch-sparse torch-geometric pyg-lib\n\n# Install compatible packages for torch 2.5.1 + CUDA 12.1\n!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install -q pyg-lib -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install -q torch-geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:14:42.210373Z","iopub.execute_input":"2025-04-15T17:14:42.210656Z","iopub.status.idle":"2025-04-15T17:15:00.125904Z","shell.execute_reply.started":"2025-04-15T17:14:42.210635Z","shell.execute_reply":"2025-04-15T17:15:00.125064Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import argparse\nimport networkx as nx\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, SAGEConv, GATConv, APPNP\nfrom torch_geometric.loader import NeighborLoader\nfrom torch_geometric.datasets import Planetoid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:15:00.126816Z","iopub.execute_input":"2025-04-15T17:15:00.127062Z","iopub.status.idle":"2025-04-15T17:15:08.766209Z","shell.execute_reply.started":"2025-04-15T17:15:00.127041Z","shell.execute_reply":"2025-04-15T17:15:08.765588Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"TODO:\nImplement teacher network architecture and training along with flags for datasets and teacher model architecture (Chanikya and Nithin)\neg: python3 train_teacher.py --dataset=cora --model=SAGE --epochs-100 --lr=0.01 . Add flags for other hyperparameters if necessary (Chanikya and Nithin)\nOther teacher model architectures - GCN, GAT, APPNP (Chanikya and Nithin + others based on availability)","metadata":{}},{"cell_type":"raw","source":"TODO: \n1. Standardise(lcc)\n2. binarize labels(dealing with multi labels)\n3. for now only worksfor cpf, extenf to ogb","metadata":{}},{"cell_type":"code","source":"def add_inductive_settings(data, spr=0.2):\n    unlabeled_indices = torch.where(data.test_mask)[0]\n\n    num_unlabeled = len(unlabeled_indices)\n    num_inductive = int(spr * num_unlabeled)\n    perm = torch.randperm(num_unlabeled)\n    inductive_indices = unlabeled_indices[perm[:num_inductive]]\n    observed_indices = unlabeled_indices[perm[num_inductive:]]\n\n    data.observed_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    data.inductive_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    data.observed_mask[observed_indices] = True\n    data.inductive_mask[inductive_indices] = True\n\n    edge_index = data.edge_index\n    src, dst = edge_index\n\n    mask = ~data.inductive_mask[src] & ~data.inductive_mask[dst]\n    data.ind_edge_index = edge_index[:, mask]\n\n    return data\n\ndef load_data(dataset, setting=\"tran\"):\n    if dataset == \"cora\":\n        dataset = Planetoid(root='./Cora', name='Cora')\n        data = dataset[0]\n        data.ind_edge_index = []\n        data.observed_mask = []\n        data.inductive_mask = []\n        test_mask = data.test_mask\n        if setting==\"ind\":\n            data = add_inductive_settings(data)\n            test_mask = data.inductive_mask\n        \n        return dataset.num_features, dataset.num_classes, data.x, data.y, data.edge_index,  data.ind_edge_index, data.train_mask, data.val_mask, test_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:40:45.273252Z","iopub.execute_input":"2025-04-15T17:40:45.273640Z","iopub.status.idle":"2025-04-15T17:40:45.280061Z","shell.execute_reply.started":"2025-04-15T17:40:45.273614Z","shell.execute_reply":"2025-04-15T17:40:45.279187Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"class GCN(nn.Module):\n    def __init__(\n        self,\n        num_layers,\n        input_dim,\n        hidden_dim,\n        output_dim,\n        dropout_ratio,\n        activation,\n        norm_type=\"none\"\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.norm_type = norm_type\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.activation = activation\n\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        if num_layers == 1:\n            self.layers.append(GCNConv(input_dim, output_dim))\n        else:\n            self.layers.append(GCNConv(input_dim, hidden_dim))\n            if norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            for _ in range(num_layers - 2):\n                self.layers.append(GCNConv(hidden_dim, hidden_dim))\n                if norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            self.layers.append(GCNConv(hidden_dim, output_dim))\n\n    def forward(self, x, edge_index):\n        h_list = []\n        h = x\n        for l, layer in enumerate(self.layers):\n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n                h_list.append(h)\n        return h_list[-1], h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:30:23.149016Z","iopub.execute_input":"2025-04-15T17:30:23.149301Z","iopub.status.idle":"2025-04-15T17:30:23.156334Z","shell.execute_reply.started":"2025-04-15T17:30:23.149279Z","shell.execute_reply":"2025-04-15T17:30:23.155629Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class SAGE(nn.Module):\n    def __init__(\n        self,\n        num_layers,\n        input_dim,\n        hidden_dim,\n        output_dim,\n        dropout_ratio,\n        activation,\n        norm_type=\"none\",\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.norm_type = norm_type\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        if num_layers == 1:\n            self.layers.append(SAGEConv(input_dim, output_dim))\n        else:\n            self.layers.append(SAGEConv(input_dim, hidden_dim))\n            if self.norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif self.norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            for _ in range(num_layers - 2):\n                self.layers.append(SAGEConv(hidden_dim, hidden_dim))\n                if self.norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif self.norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            self.layers.append(SAGEConv(hidden_dim, output_dim))\n\n    def forward(self, x, edge_index):\n        h = x\n        h_list = []\n        for l, layer in enumerate(self.layers):\n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                h_list.append(h)\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n        return h_list[-1], h\n\n\n    def inference(self, x_all, edge_index, batch_size=1024, device=\"cuda\"):\n        \"\"\"\n        Full-graph inference using mini-batches (for large graphs).\n        \"\"\"\n        from torch_geometric.loader import NeighborLoader\n\n        x = x_all.to(device)\n        for l, layer in enumerate(self.layers):\n            new_x = torch.zeros(\n                x_all.size(0),\n                self.hidden_dim if l != self.num_layers - 1 else self.output_dim,\n            ).to(device)\n\n            loader = NeighborLoader(\n                data=(x_all, edge_index),\n                input_nodes=torch.arange(x_all.size(0)),\n                num_neighbors=[-1],  # full neighbors\n                batch_size=batch_size,\n                shuffle=False\n            )\n\n            for batch in loader:\n                batch = batch.to(device)\n                h = x[batch.n_id]\n                h = layer(h, batch.edge_index)\n\n                if l != self.num_layers - 1:\n                    if self.norm_type != \"none\":\n                        h = self.norms[l](h)\n                    h = self.activation(h)\n                    h = self.dropout(h)\n\n                new_x[batch.n_id[:batch.batch_size]] = h\n\n            x = new_x\n        return x\n# For small, medium datasets few thousands, use model() in eval\n# For large ones like 100k or millions, use inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:30:24.744171Z","iopub.execute_input":"2025-04-15T17:30:24.744495Z","iopub.status.idle":"2025-04-15T17:30:24.754432Z","shell.execute_reply.started":"2025-04-15T17:30:24.744470Z","shell.execute_reply":"2025-04-15T17:30:24.753637Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"class GAT(nn.Module):\n    def __init__(\n            self,\n            num_layers,\n            input_dim,\n            hidden_dim,\n            output_dim,\n            dropout_ratio,\n            activation=F.relu,\n            num_heads=8,\n            attn_drop=0.3,\n            negative_slope=0.2,\n            residual=False,\n    ):\n        super().__init__()\n        \n        assert num_layers > 1\n\n        hidden_dim //= num_heads \n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_ratio)\n\n        heads = [num_heads] * (num_layers - 1) + [1]  \n        # heads = ([num_heads] * num_layers) + [1]\n\n        # Input layer\n        self.layers.append(\n            GATConv(\n                in_channels=input_dim,\n                out_channels=hidden_dim,\n                heads=heads[0],\n                dropout=attn_drop,\n                negative_slope=negative_slope,\n                concat=True, \n            )\n        )\n\n        # Hidden layers\n        for l in range(1, num_layers - 1):\n            self.layers.append(\n                GATConv(\n                    in_channels=hidden_dim * heads[l - 1],  \n                    out_channels=hidden_dim,\n                    heads=heads[l],\n                    dropout=attn_drop,\n                    negative_slope=negative_slope,\n                    concat=True, \n                )\n            )\n\n        # Output layer\n        self.layers.append(\n            GATConv(\n                in_channels=hidden_dim * heads[-2],  \n                out_channels=output_dim,\n                heads=heads[-1],  \n                dropout=attn_drop,\n                negative_slope=negative_slope,\n                concat=False, \n            )\n        )\n\n    def forward(self, x, edge_index):\n        h_list = []\n        h = x\n        for l, layer in enumerate(self.layers):\n            h = self.dropout(h) \n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                h = self.activation(h)  \n                h_list.append(h)\n        return h_list[-1], h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:30:25.513120Z","iopub.execute_input":"2025-04-15T17:30:25.513452Z","iopub.status.idle":"2025-04-15T17:30:25.520622Z","shell.execute_reply.started":"2025-04-15T17:30:25.513425Z","shell.execute_reply":"2025-04-15T17:30:25.519819Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class APPNP_Model(nn.Module):\n    def __init__(\n            self,\n            num_layers,\n            input_dim,\n            hidden_dim,\n            output_dim,\n            dropout_ratio,\n            activation=F.relu,\n            norm_type=\"none\",\n            edge_drop=0,\n            alpha=0.1,\n            k=10,\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.norm_type = norm_type\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        # Input layer\n        if num_layers == 1:\n            self.layers.append(nn.Linear(input_dim, output_dim))\n        else:\n            self.layers.append(nn.Linear(input_dim, hidden_dim))\n            if self.norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif self.norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            # Hidden layers\n            for _ in range(num_layers - 2):\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                if self.norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif self.norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            # Output layer\n            self.layers.append(nn.Linear(hidden_dim, output_dim))\n\n        self.propagate = APPNP(K=k, alpha=alpha, dropout=edge_drop)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for layer in self.layers:\n            if hasattr(layer, \"reset_parameters\"):\n                layer.reset_parameters()\n\n    def forward(self, x, edge_index):\n        h_list = []\n        h = x\n\n        for l, layer in enumerate(self.layers):\n            h = layer(h)\n\n            if l != self.num_layers - 1:  \n                h_list.append(h)\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n\n        h = self.propagate(h, edge_index)\n        return h_list[-1], h ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:30:26.095224Z","iopub.execute_input":"2025-04-15T17:30:26.095558Z","iopub.status.idle":"2025-04-15T17:30:26.103741Z","shell.execute_reply.started":"2025-04-15T17:30:26.095530Z","shell.execute_reply":"2025-04-15T17:30:26.102872Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def train_sage(model, loader, optimizer, criterion, device, homo=True):\n    model.train()\n    total_loss = 0\n\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        x = batch.x\n        y = batch.y[:batch.batch_size]  # Only use input nodes\n\n        if homo:\n            edge_index = batch.edge_index\n        else:\n            rel = list(batch.edge_index_dict.keys())[0]\n            edge_index = batch.edge_index_dict[rel]\n\n        _, out = model(x, edge_index)\n        out = out[:batch.batch_size]  # Only use predictions for input nodes\n\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\n\n@torch.no_grad()\ndef evaluate_sage(model, loader, device, homo=True):\n    model.eval()\n    correct = 0\n    total = 0\n\n    for batch in loader:\n        batch = batch.to(device)\n        x = batch.x\n        y = batch.y[:batch.batch_size]  # Only input nodes\n\n        if homo:\n            edge_index = batch.edge_index\n        else:\n            rel = list(batch.edge_index_dict.keys())[0]\n            edge_index = batch.edge_index_dict[rel]\n\n        _, out = model(x, edge_index)\n        out = out[:batch.batch_size]  # Only predictions for input nodes\n\n        pred = out.argmax(dim=1)\n        correct += (pred == y).sum().item()\n        total += y.size(0)\n\n    return correct / total\n\ndef train(model, data, edge_index, labels, train_mask, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()\n    _, out = model(data, edge_index)\n    loss = criterion(out[train_mask], labels[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n@torch.no_grad()\ndef evaluate(model, data, edge_index, labels, idx):\n    model.eval()\n    _, out = model(data, edge_index)\n    pred = out[idx].argmax(dim=1)\n    correct = (pred == labels[idx]).sum().item()\n    acc = correct / sum(idx)\n    return acc\n    \n# save embeddings, softmax scores tensors above in a directory\ndef save_tensors(emb_t, z_soft, output_dir):\n    torch.save(emb_t, f\"{output_dir}/embeddings.pt\")\n    torch.save(z_soft, f\"{output_dir}/label_scores.pt\")\n# Example usage\noutput_dir = \"./teacher_outputs\"\nimport os\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:31:44.553480Z","iopub.execute_input":"2025-04-15T17:31:44.553787Z","iopub.status.idle":"2025-04-15T17:31:44.563695Z","shell.execute_reply.started":"2025-04-15T17:31:44.553766Z","shell.execute_reply":"2025-04-15T17:31:44.562866Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def run_SAGE(data, num_features, num_classes, setting=\"tran\"):   \n    train_mask, val_mask, test_mask = data.train_mask, data.val_mask, data.test_mask \n    if setting == \"ind\":\n        test_mask = data.inductive_mask\n    \n    train_loader = NeighborLoader(\n        data,\n        input_nodes=train_mask,\n        num_neighbors=[5, 5],\n        batch_size=32,\n        shuffle=True\n    )\n    \n    #TODO: don't know what's val mask in ind setting\n    val_loader = NeighborLoader(\n        data,\n        input_nodes=val_mask,\n        num_neighbors=[-1, -1],\n        batch_size=32\n    )\n    \n    test_loader = NeighborLoader(\n        data,\n        input_nodes=test_mask,\n        num_neighbors=[-1, -1],\n        batch_size=32\n    )\n    \n    \n    model = SAGE(\n        num_layers=2,\n        input_dim=num_features,\n        hidden_dim=128,\n        output_dim=num_classes,\n        dropout_ratio=0,\n        activation=nn.functional.relu,\n        norm_type=\"batch\"\n    )\n    \n    device = 'cuda'\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epoch in range(1, 201):\n        loss = train_sage(model, train_loader, optimizer, criterion, device)\n        val_acc = evaluate_sage(model, val_loader, device)\n        if epoch % 10 == 0 or epoch == 1:\n            test_acc =evaluate_sage(model, test_loader, device)\n            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n\n    mb_t, z_soft = model.forward(data.x.to(device), data.edge_index.to(device))\n    return mb_t, z_soft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:31:49.184127Z","iopub.execute_input":"2025-04-15T17:31:49.184494Z","iopub.status.idle":"2025-04-15T17:31:49.191050Z","shell.execute_reply.started":"2025-04-15T17:31:49.184467Z","shell.execute_reply":"2025-04-15T17:31:49.190406Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def run_model(model, features, edge_index, labels, train_mask, val_mask, test_mask, setting=\"tran\", orig_edge_index=[]):\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    edge_index_eval = edge_index\n    if setting==\"ind\":\n        edge_index_eval = orig_edge_index\n    \n    for epoch in range(1, 200):\n        loss = train(model, features, edge_index, labels, train_mask, optimizer, criterion)\n        val_acc = evaluate(model, features, edge_index_eval, labels, val_mask)\n        if epoch % 10 == 0 or epoch == 1:\n            test_acc = evaluate(model, features, edge_index_eval, labels, test_mask)\n            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:58:11.222537Z","iopub.execute_input":"2025-04-15T17:58:11.222837Z","iopub.status.idle":"2025-04-15T17:58:11.227986Z","shell.execute_reply.started":"2025-04-15T17:58:11.222817Z","shell.execute_reply":"2025-04-15T17:58:11.227282Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"def run_GCN(num_features, num_classes, features, edge_index, labels, train_mask, val_mask, test_mask, setting=\"tran\",  orig_edge_index=[]):\n    gcn_model = GCN(\n        num_layers=3,\n        input_dim=num_features,\n        hidden_dim=64,\n        output_dim=num_classes,\n        dropout_ratio=0.8,\n        activation=nn.functional.relu,\n        norm_type=\"batch\"\n    )\n    # print(ind_edge_index.shape)\n    run_model(gcn_model, features, edge_index, labels, train_mask, val_mask, test_mask,  setting, orig_edge_index)\n\ndef run_APPNP(num_features, num_classes, features, edge_index, labels, train_mask, val_mask, test_mask,  setting=\"tran\", orig_edge_index=[]):\n    appnp_model = APPNP_Model(\n        num_layers=2,\n        input_dim=num_features,  \n        hidden_dim=128,\n        output_dim=num_classes,  \n        dropout_ratio=0.5,\n        activation=F.relu,\n    )\n    \n    run_model(appnp_model, features, edge_index, labels, train_mask, val_mask, test_mask, setting, orig_edge_index)\n\ndef run_GAT(num_features, num_classes, features, edge_index, labels, train_mask, val_mask, test_mask, setting=\"tran\", orig_edge_index=[]):\n    gat_model = GAT(\n        num_layers=2,\n        input_dim=num_features,  \n        hidden_dim=128,\n        output_dim=num_classes,       \n        dropout_ratio=0.6,\n        activation=F.relu,\n        num_heads=8,\n        attn_drop=0.3,\n        negative_slope=0.2,\n        residual=True\n    )\n    \n    run_model(gat_model, features, edge_index, labels, train_mask, val_mask, test_mask, setting, orig_edge_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:49:06.822881Z","iopub.execute_input":"2025-04-15T17:49:06.823160Z","iopub.status.idle":"2025-04-15T17:49:06.827483Z","shell.execute_reply.started":"2025-04-15T17:49:06.823139Z","shell.execute_reply":"2025-04-15T17:49:06.826747Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# Code to train models like GCN, GAT, APPNP\nsetting = \"ind\"\nnum_features, num_classes, features, labels, edge_index, ind_edge_index, train_mask, val_mask, test_mask = load_data(\"cora\", setting)\nedges = edge_index\nif setting == \"ind\":\n    edges = ind_edge_index\nrun_GAT(num_features, num_classes, features, edges, labels, train_mask, val_mask, test_mask, setting, edge_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:02:26.099035Z","iopub.execute_input":"2025-04-15T18:02:26.099380Z","iopub.status.idle":"2025-04-15T18:02:26.112441Z","shell.execute_reply.started":"2025-04-15T18:02:26.099328Z","shell.execute_reply":"2025-04-15T18:02:26.111610Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"# Code to train SAGE\nsetting = \"tran\"\ndataset = Planetoid(root='./Cora', name='Cora')\ndata = dataset[0]\ndata.ind_edge_index = []\ndata.observed_mask = []\ndata.inductive_mask = []\ntest_mask = data.test_mask\nif setting==\"ind\":\n    data = add_inductive_settings(data)\n    test_mask = data.inductive_mask\nrun_SAGE(data, dataset.num_node_features, dataset.num_classes, setting)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    parser = argparse.ArgumentParser(description=\"Teacher implementation\")\n    parser.add_argument('--num_runs', type=int, default=1, help='Number of runs')\n    parser.add_argument('--setting', type=str, choices=['trans', 'ind'], required=True, help='Setting type: trans or ind')\n    parser.add_argument('--data_path', type=str, required=True, help='Path to the dataset')\n    parser.add_argument('--model_name', type=str, default='SAGE', help='Name of the model(SAGE, GCN, GAT, APPNP)')\n    parser.add_argument('--num_layers', type=int, default=2, help='Number of layers in the model')\n    parser.add_argument('--hidden_dim', type=int, default=128, help='Hidden dimension size')\n    parser.add_argument('--drop_out', type=float, default=0, help='Dropout rate')\n    parser.add_argument('--batch_sz', type=int, default=512, help='Batch size')\n    parser.add_argument('--learning_rate', type=float, default=0.01, help='Learning rate')\n    parser.add_argument('--output_path', type=str, default='./output', help='Path to save output')\n    \n    args = parser.parse_args()\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:15:38.183223Z","iopub.status.idle":"2025-04-15T17:15:38.183529Z","shell.execute_reply":"2025-04-15T17:15:38.183407Z"}},"outputs":[],"execution_count":null}]}