{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clean uninstall just in case\n!pip uninstall -y torch-scatter torch-sparse torch-geometric pyg-lib\n\n# Install compatible packages for torch 2.5.1 + CUDA 12.1\n!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install -q pyg-lib -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install -q torch-geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:49:33.745672Z","iopub.execute_input":"2025-04-13T10:49:33.745903Z","iopub.status.idle":"2025-04-13T10:49:54.536053Z","shell.execute_reply.started":"2025-04-13T10:49:33.745881Z","shell.execute_reply":"2025-04-13T10:49:54.535092Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping pyg-lib as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import argparse\nimport networkx as nx\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n!pip install torch-geometric\nfrom torch_geometric.nn import GCNConv, SAGEConv, GATConv, APPNP\nfrom torch_geometric.loader import NeighborLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:49:54.536913Z","iopub.execute_input":"2025-04-13T10:49:54.537156Z","iopub.status.idle":"2025-04-13T10:50:09.459144Z","shell.execute_reply.started":"2025-04-13T10:49:54.537135Z","shell.execute_reply":"2025-04-13T10:50:09.458366Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.12)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.12.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2025.1.31)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch-geometric) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"TODO:\nImplement teacher network architecture and training along with flags for datasets and teacher model architecture (Chanikya and Nithin)\neg: python3 train_teacher.py --dataset=cora --model=SAGE --epochs-100 --lr=0.01 . Add flags for other hyperparameters if necessary (Chanikya and Nithin)\nOther teacher model architectures - GCN, GAT, APPNP (Chanikya and Nithin + others based on availability)","metadata":{}},{"cell_type":"code","source":"from torch_geometric.datasets import Planetoid\ndataset = Planetoid(root='./Cora', name='Cora')\n\ndata = dataset[0]\nprint(f'Dataset: {dataset}:')\nprint('======================')\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:09.460045Z","iopub.execute_input":"2025-04-13T10:50:09.460604Z","iopub.status.idle":"2025-04-13T10:50:12.476317Z","shell.execute_reply.started":"2025-04-13T10:50:09.460571Z","shell.execute_reply":"2025-04-13T10:50:12.475464Z"}},"outputs":[{"name":"stderr","text":"Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\nProcessing...\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Cora():\n======================\nNumber of graphs: 1\nNumber of features: 1433\nNumber of classes: 7\n","output_type":"stream"},{"name":"stderr","text":"Done!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"raw","source":"TODO: \n1. Standardise(lcc)\n2. binarize labels(dealing with multi labels)\n3. for now only worksfor cpf, extenf to ogb","metadata":{}},{"cell_type":"code","source":"# data.edge_index.t()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:12.478158Z","iopub.execute_input":"2025-04-13T10:50:12.478595Z","iopub.status.idle":"2025-04-13T10:50:12.481858Z","shell.execute_reply.started":"2025-04-13T10:50:12.478572Z","shell.execute_reply":"2025-04-13T10:50:12.480985Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_data(dataset):\n    if dataset == \"cora\":\n        dataset = Planetoid(root='./Cora', name='Cora')\n        data = dataset[0]\n        graph_nx = nx.Graph()\n        graph_nx.add_edges_from(data.edge_index.t().tolist())\n\n        # Adding self-loops\n        # graph_nx.add_edges_from((n, n) for n in graph_nx.nodes())\n        \n        # adj_tensor = torch.tensor(nx.to_numpy_array(graph_nx), dtype=torch.float).to('cuda')\n        features = data.x\n        labels = data.y\n\n        train_idx = data.train_mask.nonzero(as_tuple=True)[0]\n        val_idx = data.val_mask.nonzero(as_tuple=True)[0]\n        test_idx = data.test_mask.nonzero(as_tuple=True)[0]\n        \n        return data.edge_index, features, labels, data.train_mask, data.val_mask, data.test_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:12.483314Z","iopub.execute_input":"2025-04-13T10:50:12.483614Z","iopub.status.idle":"2025-04-13T10:50:12.502443Z","shell.execute_reply.started":"2025-04-13T10:50:12.483593Z","shell.execute_reply":"2025-04-13T10:50:12.501628Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"edge_index, features, labels, train_idx, val_idx, test_idx = load_data(\"cora\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:52:42.768678Z","iopub.execute_input":"2025-04-13T10:52:42.769010Z","iopub.status.idle":"2025-04-13T10:52:42.799066Z","shell.execute_reply.started":"2025-04-13T10:52:42.768989Z","shell.execute_reply":"2025-04-13T10:52:42.798455Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"GCN => num layers, hidden, input dim, out, dp, activa,  ","metadata":{}},{"cell_type":"code","source":"class GCN(nn.Module):\n    def __init__(\n        self,\n        num_layers,\n        input_dim,\n        hidden_dim,\n        output_dim,\n        dropout_ratio,\n        activation,\n        norm_type=\"none\"\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.norm_type = norm_type\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.activation = activation\n\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        if num_layers == 1:\n            self.layers.append(GCNConv(input_dim, output_dim))\n        else:\n            self.layers.append(GCNConv(input_dim, hidden_dim))\n            if norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            for _ in range(num_layers - 2):\n                self.layers.append(GCNConv(hidden_dim, hidden_dim))\n                if norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            self.layers.append(GCNConv(hidden_dim, output_dim))\n\n    def forward(self, x, edge_index):\n        h_list = []\n        h = x\n        for l, layer in enumerate(self.layers):\n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n                h_list.append(h)\n        return h_list[-1], h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:12.503257Z","iopub.execute_input":"2025-04-13T10:50:12.503547Z","iopub.status.idle":"2025-04-13T10:50:12.520080Z","shell.execute_reply.started":"2025-04-13T10:50:12.503524Z","shell.execute_reply":"2025-04-13T10:50:12.519375Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class SAGE(nn.Module):\n    def __init__(\n        self,\n        num_layers,\n        input_dim,\n        hidden_dim,\n        output_dim,\n        dropout_ratio,\n        activation,\n        norm_type=\"none\",\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.norm_type = norm_type\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        if num_layers == 1:\n            self.layers.append(SAGEConv(input_dim, output_dim))\n        else:\n            self.layers.append(SAGEConv(input_dim, hidden_dim))\n            if self.norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif self.norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            for _ in range(num_layers - 2):\n                self.layers.append(SAGEConv(hidden_dim, hidden_dim))\n                if self.norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif self.norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            self.layers.append(SAGEConv(hidden_dim, output_dim))\n\n    def forward(self, x, edge_index):\n        h = x\n        h_list = []\n        for l, layer in enumerate(self.layers):\n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                h_list.append(h)\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n        return h_list[-1], h\n\n\n    def inference(self, x_all, edge_index, batch_size=1024, device=\"cuda\"):\n        \"\"\"\n        Full-graph inference using mini-batches (for large graphs).\n        \"\"\"\n        from torch_geometric.loader import NeighborLoader\n\n        x = x_all.to(device)\n        for l, layer in enumerate(self.layers):\n            new_x = torch.zeros(\n                x_all.size(0),\n                self.hidden_dim if l != self.num_layers - 1 else self.output_dim,\n            ).to(device)\n\n            loader = NeighborLoader(\n                data=(x_all, edge_index),\n                input_nodes=torch.arange(x_all.size(0)),\n                num_neighbors=[-1],  # full neighbors\n                batch_size=batch_size,\n                shuffle=False\n            )\n\n            for batch in loader:\n                batch = batch.to(device)\n                h = x[batch.n_id]\n                h = layer(h, batch.edge_index)\n\n                if l != self.num_layers - 1:\n                    if self.norm_type != \"none\":\n                        h = self.norms[l](h)\n                    h = self.activation(h)\n                    h = self.dropout(h)\n\n                new_x[batch.n_id[:batch.batch_size]] = h\n\n            x = new_x\n        return x\n# For small, medium datasets few thousands, use model() in eval\n# For large ones like 100k or millions, use inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:12.520871Z","iopub.execute_input":"2025-04-13T10:50:12.521103Z","iopub.status.idle":"2025-04-13T10:50:12.550522Z","shell.execute_reply.started":"2025-04-13T10:50:12.521084Z","shell.execute_reply":"2025-04-13T10:50:12.549777Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class GAT(nn.Module):\n    def __init__(\n            self,\n            num_layers,\n            input_dim,\n            hidden_dim,\n            output_dim,\n            dropout_ratio,\n            activation=F.relu,\n            num_heads=8,\n            attn_drop=0.3,\n            negative_slope=0.2,\n            residual=False,\n    ):\n        super().__init__()\n        \n        assert num_layers > 1\n\n        hidden_dim //= num_heads \n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_ratio)\n\n        heads = [num_heads] * (num_layers - 1) + [1]  \n        # heads = ([num_heads] * num_layers) + [1]\n\n        # Input layer\n        self.layers.append(\n            GATConv(\n                in_channels=input_dim,\n                out_channels=hidden_dim,\n                heads=heads[0],\n                dropout=attn_drop,\n                negative_slope=negative_slope,\n                concat=True, \n            )\n        )\n\n        # Hidden layers\n        for l in range(1, num_layers - 1):\n            self.layers.append(\n                GATConv(\n                    in_channels=hidden_dim * heads[l - 1],  \n                    out_channels=hidden_dim,\n                    heads=heads[l],\n                    dropout=attn_drop,\n                    negative_slope=negative_slope,\n                    concat=True, \n                )\n            )\n\n        # Output layer\n        self.layers.append(\n            GATConv(\n                in_channels=hidden_dim * heads[-2],  \n                out_channels=output_dim,\n                heads=heads[-1],  \n                dropout=attn_drop,\n                negative_slope=negative_slope,\n                concat=False, \n            )\n        )\n\n    def forward(self, x, edge_index):\n        h_list = []\n        h = x\n        for l, layer in enumerate(self.layers):\n            h = self.dropout(h) \n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                h = self.activation(h)  \n                h_list.append(h)\n        return h_list[-1], h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:12.551382Z","iopub.execute_input":"2025-04-13T10:50:12.551684Z","iopub.status.idle":"2025-04-13T10:50:12.573378Z","shell.execute_reply.started":"2025-04-13T10:50:12.551662Z","shell.execute_reply":"2025-04-13T10:50:12.572785Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class APPNP_Model(nn.Module):\n    def __init__(\n            self,\n            num_layers,\n            input_dim,\n            hidden_dim,\n            output_dim,\n            dropout_ratio,\n            activation=F.relu,\n            norm_type=\"none\",\n            edge_drop=0,\n            alpha=0.1,\n            k=10,\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.norm_type = norm_type\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        # Input layer\n        if num_layers == 1:\n            self.layers.append(nn.Linear(input_dim, output_dim))\n        else:\n            self.layers.append(nn.Linear(input_dim, hidden_dim))\n            if self.norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif self.norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            # Hidden layers\n            for _ in range(num_layers - 2):\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                if self.norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif self.norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            # Output layer\n            self.layers.append(nn.Linear(hidden_dim, output_dim))\n\n        self.propagate = APPNP(K=k, alpha=alpha, dropout=edge_drop)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for layer in self.layers:\n            if hasattr(layer, \"reset_parameters\"):\n                layer.reset_parameters()\n\n    def forward(self, x, edge_index):\n        h_list = []\n        h = x\n\n        for l, layer in enumerate(self.layers):\n            h = layer(h)\n\n            if l != self.num_layers - 1:  \n                h_list.append(h)\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n\n        h = self.propagate(h, edge_index)\n        return h_list[-1], h ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:12.574187Z","iopub.execute_input":"2025-04-13T10:50:12.574426Z","iopub.status.idle":"2025-04-13T10:50:12.747893Z","shell.execute_reply.started":"2025-04-13T10:50:12.574407Z","shell.execute_reply":"2025-04-13T10:50:12.747182Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train_sage(model, loader, optimizer, criterion, device, homo=True):\n    model.train()\n    total_loss = 0\n\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        x = batch.x\n        y = batch.y[:batch.batch_size]  # Only use input nodes\n\n        if homo:\n            edge_index = batch.edge_index\n        else:\n            rel = list(batch.edge_index_dict.keys())[0]\n            edge_index = batch.edge_index_dict[rel]\n\n        _, out = model(x, edge_index)\n        out = out[:batch.batch_size]  # Only use predictions for input nodes\n\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\n\n@torch.no_grad()\ndef evaluate_sage(model, loader, device, homo=True):\n    model.eval()\n    correct = 0\n    total = 0\n\n    for batch in loader:\n        batch = batch.to(device)\n        x = batch.x\n        y = batch.y[:batch.batch_size]  # Only input nodes\n\n        if homo:\n            edge_index = batch.edge_index\n        else:\n            rel = list(batch.edge_index_dict.keys())[0]\n            edge_index = batch.edge_index_dict[rel]\n\n        _, out = model(x, edge_index)\n        out = out[:batch.batch_size]  # Only predictions for input nodes\n\n        pred = out.argmax(dim=1)\n        correct += (pred == y).sum().item()\n        total += y.size(0)\n\n    return correct / total\n\ndef train(model, data, edge_index, labels, train_idx, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()\n    _, out = model(data, edge_index)\n    loss = criterion(out[train_idx], labels[train_idx])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n@torch.no_grad()\ndef evaluate(model, data, edge_index, labels, idx):\n    model.eval()\n    _, out = model(data, edge_index)\n    pred = out[idx].argmax(dim=1)\n    correct = (pred == labels[idx]).sum().item()\n    acc = correct / sum(idx)\n    return acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:12.748649Z","iopub.execute_input":"2025-04-13T10:50:12.748919Z","iopub.status.idle":"2025-04-13T10:50:12.764938Z","shell.execute_reply.started":"2025-04-13T10:50:12.748891Z","shell.execute_reply":"2025-04-13T10:50:12.764309Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"    \ndef run_SAGE():    \n    train_loader = NeighborLoader(\n        data,\n        input_nodes=data.train_mask,\n        num_neighbors=[15, 10],\n        batch_size=128,\n        shuffle=True\n    )\n    \n    val_loader = NeighborLoader(\n        data,\n        input_nodes=data.val_mask,\n        num_neighbors=[-1, -1],\n        batch_size=128\n    )\n    \n    test_loader = NeighborLoader(\n        data,\n        input_nodes=data.test_mask,\n        num_neighbors=[-1, -1],\n        batch_size=128\n    )\n    \n    \n    model = SAGE(\n        num_layers=2,\n        input_dim=dataset.num_node_features,\n        hidden_dim=128,\n        output_dim=dataset.num_classes,\n        dropout_ratio=0,\n        activation=nn.functional.relu,\n        norm_type=\"batch\"\n    )\n    \n    device = 'cuda'\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epoch in range(1, 201):\n        loss = train_sage(model, train_loader, optimizer, criterion, device)\n        val_acc = evaluate_sage(model, val_loader, device)\n        if epoch % 10 == 0 or epoch == 1:\n            test_acc =evaluate_sage(model, test_loader, device)\n            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:54:55.446000Z","iopub.execute_input":"2025-04-13T10:54:55.446324Z","iopub.status.idle":"2025-04-13T10:54:55.452741Z","shell.execute_reply.started":"2025-04-13T10:54:55.446302Z","shell.execute_reply":"2025-04-13T10:54:55.451914Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def run_GCN():\n    model = GCN(\n        num_layers=3,\n        input_dim=dataset.num_node_features,\n        hidden_dim=64,\n        output_dim=dataset.num_classes,\n        dropout_ratio=0.8,\n        activation=nn.functional.relu,\n        norm_type=\"batch\"\n    )\n    # model = GCN1(dataset.num_node_features, 64, dataset.num_classes)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epoch in range(1, 150):\n        loss = train(model, features, edge_index, labels, train_idx, optimizer, criterion)\n        val_acc = evaluate(model, features, edge_index, labels, val_idx)\n        if epoch % 10 == 0 or epoch == 1:\n            test_acc = evaluate(model, features, edge_index, labels, test_idx)\n            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:38.981593Z","iopub.execute_input":"2025-04-13T10:50:38.981910Z","iopub.status.idle":"2025-04-13T10:50:38.987643Z","shell.execute_reply.started":"2025-04-13T10:50:38.981887Z","shell.execute_reply":"2025-04-13T10:50:38.986817Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def run_APPNP():\n    model = APPNP_Model(\n        num_layers=2,\n        input_dim=dataset.num_node_features,  \n        hidden_dim=128,\n        output_dim=dataset.num_classes,  \n        dropout_ratio=0.5,\n        activation=F.relu,\n    )\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epoch in range(1, 100):\n        loss = train(model, features, edge_index, labels, train_idx, optimizer, criterion)\n        val_acc = evaluate(model, features, edge_index, labels, val_idx)\n        if epoch % 10 == 0 or epoch == 1:\n            test_acc = evaluate(model, features, edge_index, labels, test_idx)\n            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:39.317491Z","iopub.execute_input":"2025-04-13T10:50:39.317761Z","iopub.status.idle":"2025-04-13T10:50:39.322838Z","shell.execute_reply.started":"2025-04-13T10:50:39.317740Z","shell.execute_reply":"2025-04-13T10:50:39.322072Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def run_GAT():\n    model = GAT(\n        num_layers=2,\n        input_dim=dataset.num_node_features,  \n        hidden_dim=128,\n        output_dim=dataset.num_classes,       \n        dropout_ratio=0.6,\n        activation=F.relu,\n        num_heads=8,\n        attn_drop=0.3,\n        negative_slope=0.2,\n        residual=True\n    )\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epoch in range(1, 150):\n        loss = train(model, features, edge_index, labels, train_idx, optimizer, criterion)\n        val_acc = evaluate(model, features, edge_index, labels, val_idx)\n        if epoch % 10 == 0 or epoch == 1:\n            test_acc = evaluate(model, features, edge_index, labels, test_idx)\n            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:41.290969Z","iopub.execute_input":"2025-04-13T10:50:41.291274Z","iopub.status.idle":"2025-04-13T10:50:41.296907Z","shell.execute_reply.started":"2025-04-13T10:50:41.291251Z","shell.execute_reply":"2025-04-13T10:50:41.295921Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"run_SAGE()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:55:00.408872Z","iopub.execute_input":"2025-04-13T10:55:00.409162Z","iopub.status.idle":"2025-04-13T10:55:07.143494Z","shell.execute_reply.started":"2025-04-13T10:55:00.409140Z","shell.execute_reply":"2025-04-13T10:55:07.142715Z"}},"outputs":[{"name":"stdout","text":"Epoch 001 | Loss: 1.8793 | Val Acc: 0.2860 | Test Acc: 0.2930\nEpoch 010 | Loss: 0.0134 | Val Acc: 0.7280 | Test Acc: 0.7510\nEpoch 020 | Loss: 0.0026 | Val Acc: 0.7580 | Test Acc: 0.7640\nEpoch 030 | Loss: 0.0020 | Val Acc: 0.7520 | Test Acc: 0.7630\nEpoch 040 | Loss: 0.0016 | Val Acc: 0.7620 | Test Acc: 0.7670\nEpoch 050 | Loss: 0.0016 | Val Acc: 0.7580 | Test Acc: 0.7710\nEpoch 060 | Loss: 0.0009 | Val Acc: 0.7620 | Test Acc: 0.7760\nEpoch 070 | Loss: 0.0008 | Val Acc: 0.7560 | Test Acc: 0.7680\nEpoch 080 | Loss: 0.0007 | Val Acc: 0.7600 | Test Acc: 0.7710\nEpoch 090 | Loss: 0.0008 | Val Acc: 0.7580 | Test Acc: 0.7750\nEpoch 100 | Loss: 0.0008 | Val Acc: 0.7640 | Test Acc: 0.7720\nEpoch 110 | Loss: 0.0006 | Val Acc: 0.7580 | Test Acc: 0.7680\nEpoch 120 | Loss: 0.0007 | Val Acc: 0.7580 | Test Acc: 0.7730\nEpoch 130 | Loss: 0.0006 | Val Acc: 0.7520 | Test Acc: 0.7760\nEpoch 140 | Loss: 0.0006 | Val Acc: 0.7580 | Test Acc: 0.7740\nEpoch 150 | Loss: 0.0005 | Val Acc: 0.7580 | Test Acc: 0.7740\nEpoch 160 | Loss: 0.0006 | Val Acc: 0.7500 | Test Acc: 0.7760\nEpoch 170 | Loss: 0.0006 | Val Acc: 0.7580 | Test Acc: 0.7730\nEpoch 180 | Loss: 0.0006 | Val Acc: 0.7500 | Test Acc: 0.7720\nEpoch 190 | Loss: 0.0009 | Val Acc: 0.7480 | Test Acc: 0.7720\nEpoch 200 | Loss: 0.0005 | Val Acc: 0.7500 | Test Acc: 0.7790\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"class Teacher:\n    def __init__(self, args):\n        self.args = args\n        pass\n    def graph_split():\n        pass\n    def train_transductive():\n        pass\n    def train_inductive():\n        pass\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:12.792685Z","iopub.status.idle":"2025-04-13T10:50:12.792985Z","shell.execute_reply":"2025-04-13T10:50:12.792879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    parser = argparse.ArgumentParser(description=\"Teacher implementation\")\n    parser.add_argument('--num_runs', type=int, default=1, help='Number of runs')\n    parser.add_argument('--setting', type=str, choices=['trans', 'ind'], required=True, help='Setting type: trans or ind')\n    parser.add_argument('--data_path', type=str, required=True, help='Path to the dataset')\n    parser.add_argument('--model_name', type=str, default='SAGE', help='Name of the model(SAGE, GCN, GAT, APPNP)')\n    parser.add_argument('--num_layers', type=int, default=2, help='Number of layers in the model')\n    parser.add_argument('--hidden_dim', type=int, default=128, help='Hidden dimension size')\n    parser.add_argument('--drop_out', type=float, default=0, help='Dropout rate')\n    parser.add_argument('--batch_sz', type=int, default=512, help='Batch size')\n    parser.add_argument('--learning_rate', type=float, default=0.01, help='Learning rate')\n    parser.add_argument('--output_path', type=str, default='./output', help='Path to save output')\n    \n    args = parser.parse_args()\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:12.793790Z","iopub.status.idle":"2025-04-13T10:50:12.794185Z","shell.execute_reply":"2025-04-13T10:50:12.794005Z"}},"outputs":[],"execution_count":null}]}