{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11431879,"sourceType":"datasetVersion","datasetId":7160029}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clean uninstall just in case\n# !pip uninstall -y torch-scatter torch-sparse torch-geometric pyg-lib\n\n# Install compatible packages for torch 2.5.1 + CUDA 12.1\n!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install -q pyg-lib -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n!pip install -q torch-geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T07:19:22.422098Z","iopub.execute_input":"2025-04-16T07:19:22.422331Z","iopub.status.idle":"2025-04-16T07:19:36.797051Z","shell.execute_reply.started":"2025-04-16T07:19:22.422311Z","shell.execute_reply":"2025-04-16T07:19:36.795755Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import argparse\nimport yaml\nimport networkx as nx\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, SAGEConv, GATConv, APPNP\nfrom torch_geometric.loader import NeighborLoader\nfrom torch_geometric.datasets import Planetoid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T07:19:36.798067Z","iopub.execute_input":"2025-04-16T07:19:36.798296Z","iopub.status.idle":"2025-04-16T07:19:36.803711Z","shell.execute_reply.started":"2025-04-16T07:19:36.798274Z","shell.execute_reply":"2025-04-16T07:19:36.802617Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"TODO:\nImplement teacher network architecture and training along with flags for datasets and teacher model architecture (Chanikya and Nithin)\neg: python3 train_teacher.py --dataset=cora --model=SAGE --epochs-100 --lr=0.01 . Add flags for other hyperparameters if necessary (Chanikya and Nithin)\nOther teacher model architectures - GCN, GAT, APPNP (Chanikya and Nithin + others based on availability)","metadata":{}},{"cell_type":"raw","source":"TODO: \n1. Standardise(lcc)\n2. binarize labels(dealing with multi labels)\n3. for now only worksfor cpf, extenf to ogb","metadata":{}},{"cell_type":"code","source":"def add_inductive_settings(data, spr=0.2, seed=42):\n    unlabeled_indices = torch.where(data.test_mask)[0]\n\n    num_unlabeled = len(unlabeled_indices)\n    num_inductive = int(spr * num_unlabeled)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    perm = torch.randperm(num_unlabeled)\n    inductive_indices = unlabeled_indices[perm[:num_inductive]]\n    observed_indices = unlabeled_indices[perm[num_inductive:]]\n\n    data.observed_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    data.inductive_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n    data.observed_mask[observed_indices] = True\n    data.inductive_mask[inductive_indices] = True\n\n    edge_index = data.edge_index\n    src, dst = edge_index\n\n    mask = ~data.inductive_mask[src] & ~data.inductive_mask[dst]\n    data.ind_edge_index = edge_index[:, mask]\n\n    return data\n\ndef load_data(dataset, setting=\"tran\"):\n        data = dataset[0]\n        data.ind_edge_index = []\n        data.observed_mask = []\n        data.inductive_mask = []\n        test_mask = data.test_mask\n        if setting==\"ind\":\n            data = add_inductive_settings(data)\n            test_mask = data.inductive_mask\n        \n        return dataset.num_features, dataset.num_classes, data.x, data.y, data.edge_index,  data.ind_edge_index, data.train_mask, data.val_mask, test_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:35:25.794220Z","iopub.execute_input":"2025-04-16T08:35:25.794573Z","iopub.status.idle":"2025-04-16T08:35:25.801924Z","shell.execute_reply.started":"2025-04-16T08:35:25.794541Z","shell.execute_reply":"2025-04-16T08:35:25.800749Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"class GCN(nn.Module):\n    def __init__(\n        self,\n        num_layers,\n        input_dim,\n        hidden_dim,\n        output_dim,\n        dropout_ratio,\n        activation,\n        norm_type=\"none\"\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.norm_type = norm_type\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.activation = activation\n\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        if num_layers == 1:\n            self.layers.append(GCNConv(input_dim, output_dim))\n        else:\n            self.layers.append(GCNConv(input_dim, hidden_dim))\n            if norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            for _ in range(num_layers - 2):\n                self.layers.append(GCNConv(hidden_dim, hidden_dim))\n                if norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            self.layers.append(GCNConv(hidden_dim, output_dim))\n\n    def forward(self, x, edge_index):\n        h_list = []\n        h = x\n        for l, layer in enumerate(self.layers):\n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n                h_list.append(h)\n        return h_list[-1], h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T07:19:36.819020Z","iopub.execute_input":"2025-04-16T07:19:36.819277Z","iopub.status.idle":"2025-04-16T07:19:36.830292Z","shell.execute_reply.started":"2025-04-16T07:19:36.819246Z","shell.execute_reply":"2025-04-16T07:19:36.829500Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class SAGE(nn.Module):\n    def __init__(\n        self,\n        num_layers,\n        input_dim,\n        hidden_dim,\n        output_dim,\n        dropout_ratio,\n        activation,\n        norm_type=\"none\",\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.norm_type = norm_type\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        if num_layers == 1:\n            self.layers.append(SAGEConv(input_dim, output_dim))\n        else:\n            self.layers.append(SAGEConv(input_dim, hidden_dim))\n            if self.norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif self.norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            for _ in range(num_layers - 2):\n                self.layers.append(SAGEConv(hidden_dim, hidden_dim))\n                if self.norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif self.norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            self.layers.append(SAGEConv(hidden_dim, output_dim))\n\n    def forward(self, x, edge_index):\n        h = x\n        h_list = []\n        for l, layer in enumerate(self.layers):\n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                h_list.append(h)\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n        return h_list[-1], h\n\n\n    def inference(self, x_all, edge_index, batch_size=1024, device=\"cuda\"):\n        \"\"\"\n        Full-graph inference using mini-batches (for large graphs).\n        \"\"\"\n        from torch_geometric.loader import NeighborLoader\n\n        x = x_all.to(device)\n        for l, layer in enumerate(self.layers):\n            new_x = torch.zeros(\n                x_all.size(0),\n                self.hidden_dim if l != self.num_layers - 1 else self.output_dim,\n            ).to(device)\n\n            loader = NeighborLoader(\n                data=(x_all, edge_index),\n                input_nodes=torch.arange(x_all.size(0)),\n                num_neighbors=[-1],  # full neighbors\n                batch_size=batch_size,\n                shuffle=False\n            )\n\n            for batch in loader:\n                batch = batch.to(device)\n                h = x[batch.n_id]\n                h = layer(h, batch.edge_index)\n\n                if l != self.num_layers - 1:\n                    if self.norm_type != \"none\":\n                        h = self.norms[l](h)\n                    h = self.activation(h)\n                    h = self.dropout(h)\n\n                new_x[batch.n_id[:batch.batch_size]] = h\n\n            x = new_x\n        return x\n# For small, medium datasets few thousands, use model() in eval\n# For large ones like 100k or millions, use inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T07:19:36.831269Z","iopub.execute_input":"2025-04-16T07:19:36.831538Z","iopub.status.idle":"2025-04-16T07:19:36.846311Z","shell.execute_reply.started":"2025-04-16T07:19:36.831516Z","shell.execute_reply":"2025-04-16T07:19:36.845535Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class GAT(nn.Module):\n    def __init__(\n            self,\n            num_layers,\n            input_dim,\n            hidden_dim,\n            output_dim,\n            dropout_ratio,\n            activation=F.relu,\n            num_heads=8,\n            attn_drop=0.3,\n            negative_slope=0.2,\n            residual=False,\n    ):\n        super().__init__()\n        \n        assert num_layers > 1\n\n        hidden_dim //= num_heads \n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_ratio)\n\n        heads = [num_heads] * (num_layers - 1) + [1]  \n        # heads = ([num_heads] * num_layers) + [1]\n\n        # Input layer\n        self.layers.append(\n            GATConv(\n                in_channels=input_dim,\n                out_channels=hidden_dim,\n                heads=heads[0],\n                dropout=attn_drop,\n                negative_slope=negative_slope,\n                concat=True, \n            )\n        )\n\n        # Hidden layers\n        for l in range(1, num_layers - 1):\n            self.layers.append(\n                GATConv(\n                    in_channels=hidden_dim * heads[l - 1],  \n                    out_channels=hidden_dim,\n                    heads=heads[l],\n                    dropout=attn_drop,\n                    negative_slope=negative_slope,\n                    concat=True, \n                )\n            )\n\n        # Output layer\n        self.layers.append(\n            GATConv(\n                in_channels=hidden_dim * heads[-2],  \n                out_channels=output_dim,\n                heads=heads[-1],  \n                dropout=attn_drop,\n                negative_slope=negative_slope,\n                concat=False, \n            )\n        )\n\n    def forward(self, x, edge_index):\n        h_list = []\n        h = x\n        for l, layer in enumerate(self.layers):\n            h = self.dropout(h) \n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                h = self.activation(h)  \n                h_list.append(h)\n        return h_list[-1], h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T07:19:36.847312Z","iopub.execute_input":"2025-04-16T07:19:36.847559Z","iopub.status.idle":"2025-04-16T07:19:36.861540Z","shell.execute_reply.started":"2025-04-16T07:19:36.847540Z","shell.execute_reply":"2025-04-16T07:19:36.860556Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class APPNP_Model(nn.Module):\n    def __init__(\n            self,\n            num_layers,\n            input_dim,\n            hidden_dim,\n            output_dim,\n            dropout_ratio,\n            activation=F.relu,\n            norm_type=\"none\",\n            edge_drop=0,\n            alpha=0.1,\n            k=10,\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.norm_type = norm_type\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        # Input layer\n        if num_layers == 1:\n            self.layers.append(nn.Linear(input_dim, output_dim))\n        else:\n            self.layers.append(nn.Linear(input_dim, hidden_dim))\n            if self.norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif self.norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            # Hidden layers\n            for _ in range(num_layers - 2):\n                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n                if self.norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif self.norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            # Output layer\n            self.layers.append(nn.Linear(hidden_dim, output_dim))\n\n        self.propagate = APPNP(K=k, alpha=alpha, dropout=edge_drop)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for layer in self.layers:\n            if hasattr(layer, \"reset_parameters\"):\n                layer.reset_parameters()\n\n    def forward(self, x, edge_index):\n        h_list = []\n        h = x\n\n        for l, layer in enumerate(self.layers):\n            h = layer(h)\n\n            if l != self.num_layers - 1:  \n                h_list.append(h)\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n\n        h = self.propagate(h, edge_index)\n        return h_list[-1], h ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T07:19:36.865205Z","iopub.execute_input":"2025-04-16T07:19:36.865519Z","iopub.status.idle":"2025-04-16T07:19:36.875652Z","shell.execute_reply.started":"2025-04-16T07:19:36.865490Z","shell.execute_reply":"2025-04-16T07:19:36.874813Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def train_sage(model, loader, optimizer, criterion, device, homo=True):\n    model.train()\n    total_loss = 0\n\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        x = batch.x\n        y = batch.y[:batch.batch_size]  # Only use input nodes\n\n        if homo:\n            edge_index = batch.edge_index\n        else:\n            rel = list(batch.edge_index_dict.keys())[0]\n            edge_index = batch.edge_index_dict[rel]\n\n        _, out = model(x, edge_index)\n        out = out[:batch.batch_size]  # Only use predictions for input nodes\n\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\n\n@torch.no_grad()\ndef evaluate_sage(model, loader, device, homo=True):\n    model.eval()\n    correct = 0\n    total = 0\n\n    for batch in loader:\n        batch = batch.to(device)\n        x = batch.x\n        y = batch.y[:batch.batch_size]  # Only input nodes\n\n        if homo:\n            edge_index = batch.edge_index\n        else:\n            rel = list(batch.edge_index_dict.keys())[0]\n            edge_index = batch.edge_index_dict[rel]\n\n        _, out = model(x, edge_index)\n        out = out[:batch.batch_size]  # Only predictions for input nodes\n\n        pred = out.argmax(dim=1)\n        correct += (pred == y).sum().item()\n        total += y.size(0)\n\n    return correct / total\n\ndef train(model, data, edge_index, labels, train_mask, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()\n    _, out = model(data, edge_index)\n    loss = criterion(out[train_mask], labels[train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n@torch.no_grad()\ndef evaluate(model, data, edge_index, labels, idx):\n    model.eval()\n    _, out = model(data, edge_index)\n    pred = out[idx].argmax(dim=1)\n    correct = (pred == labels[idx]).sum().item()\n    acc = correct / sum(idx)\n    return acc\n    \n# save embeddings, softmax scores tensors above in a directory\ndef save_tensors(emb_t, z_soft, output_dir):\n    torch.save(emb_t, f\"{output_dir}/embeddings.pt\")\n    torch.save(z_soft, f\"{output_dir}/label_scores.pt\")\n# Example usage\noutput_dir = \"./teacher_outputs\"\nimport os\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T07:19:36.876979Z","iopub.execute_input":"2025-04-16T07:19:36.877213Z","iopub.status.idle":"2025-04-16T07:19:36.891178Z","shell.execute_reply.started":"2025-04-16T07:19:36.877181Z","shell.execute_reply":"2025-04-16T07:19:36.890259Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def run_SAGE(args, data, num_features, num_classes, setting=\"tran\"):   \n    train_mask, val_mask, test_mask = data.train_mask, data.val_mask, data.test_mask \n    if setting == \"ind\":\n        test_mask = data.inductive_mask\n\n    fan_out = []\n    for i in args.fan_out.split(\",\"):\n        fan_out.append(int(i))\n    \n    train_loader = NeighborLoader(\n        data,\n        input_nodes=train_mask,\n        num_neighbors=fan_out,\n        batch_size=32,\n        shuffle=True\n    )\n    \n    #TODO: don't know what's val mask in ind setting\n    val_loader = NeighborLoader(\n        data,\n        input_nodes=val_mask,\n        num_neighbors=[-1, -1],\n        batch_size=32\n    )\n    \n    test_loader = NeighborLoader(\n        data,\n        input_nodes=test_mask,\n        num_neighbors=[-1, -1],\n        batch_size=32\n    )\n    \n    \n    model = SAGE(\n        num_layers=args.num_layers,\n        input_dim=num_features,\n        hidden_dim=args.hidden_dim,\n        output_dim=num_classes,\n        dropout_ratio=args.dropout_ratio,\n        activation=nn.functional.relu,\n        norm_type=\"batch\"\n    )\n    \n    device = 'cuda'\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epoch in range(1, 101):\n        loss = train_sage(model, train_loader, optimizer, criterion, device)\n        val_acc = evaluate_sage(model, val_loader, device)\n        if epoch % 10 == 0 or epoch == 1:\n            test_acc =evaluate_sage(model, test_loader, device)\n            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n\n    mb_t, z_soft = model.forward(data.x.to(device), data.edge_index.to(device))\n    return mb_t, z_soft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:38:21.900507Z","iopub.execute_input":"2025-04-16T08:38:21.900863Z","iopub.status.idle":"2025-04-16T08:38:21.908473Z","shell.execute_reply.started":"2025-04-16T08:38:21.900840Z","shell.execute_reply":"2025-04-16T08:38:21.907470Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"def run_model(args, model, features, edge_index, labels, train_mask, val_mask, test_mask, setting=\"tran\", orig_edge_index=[]):\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    edge_index_eval = edge_index\n    if setting==\"ind\":\n        edge_index_eval = orig_edge_index\n    \n    for epoch in range(1, 200):\n        loss = train(model, features, edge_index, labels, train_mask, optimizer, criterion)\n        val_acc = evaluate(model, features, edge_index_eval, labels, val_mask)\n        if epoch % 10 == 0 or epoch == 1:\n            test_acc = evaluate(model, features, edge_index_eval, labels, test_mask)\n            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:05:33.442582Z","iopub.execute_input":"2025-04-16T08:05:33.442939Z","iopub.status.idle":"2025-04-16T08:05:33.449693Z","shell.execute_reply.started":"2025-04-16T08:05:33.442915Z","shell.execute_reply":"2025-04-16T08:05:33.448635Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"def run_GCN(args, num_features, num_classes, features, edge_index, labels, train_mask, val_mask, test_mask, setting=\"tran\",  orig_edge_index=[]):\n    gcn_model = GCN(\n        num_layers=args.num_layers,\n        input_dim=num_features,\n        hidden_dim=args.hidden_dim,\n        output_dim=num_classes,\n        dropout_ratio=args.dropout_ratio,\n        activation=nn.functional.relu,\n        norm_type=\"batch\"\n    )\n    run_model(args, gcn_model, features, edge_index, labels, train_mask, val_mask, test_mask,  setting, orig_edge_index)\n\ndef run_APPNP(args, num_features, num_classes, features, edge_index, labels, train_mask, val_mask, test_mask,  setting=\"tran\", orig_edge_index=[]):\n    appnp_model = APPNP_Model(\n        num_layers=args.num_layers,\n        input_dim=num_features,  \n        hidden_dim=args.hidden_dim,\n        output_dim=num_classes,  \n        dropout_ratio=args.dropout_ratio,\n        activation=F.relu,\n    )\n    \n    run_model(args, appnp_model, features, edge_index, labels, train_mask, val_mask, test_mask, setting, orig_edge_index)\n\ndef run_GAT(args, num_features, num_classes, features, edge_index, labels, train_mask, val_mask, test_mask, setting=\"tran\", orig_edge_index=[]):\n    gat_model = GAT(\n        num_layers=args.num_layers,\n        input_dim=num_features,  \n        hidden_dim=args.hidden_dim,\n        output_dim=num_classes,       \n        dropout_ratio=args.dropout_ratio,\n        activation=F.relu,\n        num_heads=args.num_heads,\n        attn_drop=args.attn_dropout_ratio,\n        negative_slope=0.2,\n        residual=True\n    )\n    \n    run_model(args, gat_model, features, edge_index, labels, train_mask, val_mask, test_mask, setting, orig_edge_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:05:36.945430Z","iopub.execute_input":"2025-04-16T08:05:36.945801Z","iopub.status.idle":"2025-04-16T08:05:36.952902Z","shell.execute_reply.started":"2025-04-16T08:05:36.945778Z","shell.execute_reply":"2025-04-16T08:05:36.952022Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def run(args, model_name, dataset_name, setting=\"tran\"):\n    if dataset_name in ['Cora', 'Citeseer', 'Pubmed']:  \n        dataset = Planetoid(root=os.path.join('/tmp', dataset_name.lower()), name=dataset_name)\n        if model_name==\"SAGE\":\n            data = dataset[0]\n            data.ind_edge_index = []\n            data.observed_mask = []\n            data.inductive_mask = []\n            test_mask = data.test_mask\n            if setting==\"ind\":\n                data = add_inductive_settings(data)\n                test_mask = data.inductive_mask\n            run_SAGE(args, data, dataset.num_node_features, dataset.num_classes, setting)\n        else:\n            num_features, num_classes, features, labels, edge_index, ind_edge_index, train_mask, val_mask, test_mask = load_data(dataset, setting)\n            edges = edge_index\n            if setting == \"ind\":\n                edges = ind_edge_index\n            if model_name == \"GAT\":\n                run_GAT(args, num_features, num_classes, features, edges, labels, train_mask, val_mask, test_mask, setting, edge_index)\n            elif model_name == \"GCN\":\n                run_GCN(args, num_features, num_classes, features, edges, labels, train_mask, val_mask, test_mask, setting, edge_index)\n            elif model_name == \"APPNP\":\n                run_APPNP(args, num_features, num_classes, features, edges, labels, train_mask, val_mask, test_mask, setting, edge_index)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:12:43.600061Z","iopub.execute_input":"2025-04-16T08:12:43.600380Z","iopub.status.idle":"2025-04-16T08:12:43.607332Z","shell.execute_reply.started":"2025-04-16T08:12:43.600355Z","shell.execute_reply":"2025-04-16T08:12:43.606537Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"import yaml\ndef load_config(path):\n    with open(path, 'r') as f:\n        return yaml.safe_load(f)\ndef merge_args_with_config(args, config):\n    # Start with global\n    merged = dict(config.get('global', {}))\n    \n    # Add dataset + model-specific settings\n    dataset_cfg = config.get(args.dataset, {})\n    model_cfg = dataset_cfg.get(args.model, {})\n    \n    merged.update(model_cfg)\n    \n    # Add back dataset and model name\n    merged['dataset'] = args.dataset\n    merged['model'] = args.model\n    \n    return merged\n\nconfig = load_config('/kaggle/input/config-tran/tran.conf.yaml')\n\nmodel = 'SAGE'\ndataset = 'Pubmed'\n\nargs = {'dataset': dataset.lower(), 'model': model}\nfinal_args = merge_args_with_config(Namespace(**args), config)\n\n# TODO: Need to do something for default values\nif 'learning_rate' not in final_args.keys():\n    final_args['learning_rate'] = 0.01\n\nprint(args)\nrun(Namespace(**final_args), model,dataset , 'tran')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:41:24.204581Z","iopub.execute_input":"2025-04-16T08:41:24.204934Z","iopub.status.idle":"2025-04-16T08:41:24.240601Z","shell.execute_reply.started":"2025-04-16T08:41:24.204906Z","shell.execute_reply":"2025-04-16T08:41:24.239770Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"def main():\n    parser = argparse.ArgumentParser(description=\"Teacher implementation\")\n    parser.add_argument('--num_runs', type=int, default=1, help='Number of runs')\n    parser.add_argument('--setting', type=str, choices=['trans', 'ind'], required=True, help='Setting type: trans or ind')\n    parser.add_argument('--data_path', type=str, required=True, help='Path to the dataset')\n    parser.add_argument('--model_name', type=str, default='SAGE', help='Name of the model(SAGE, GCN, GAT, APPNP)')\n    parser.add_argument('--num_layers', type=int, default=2, help='Number of layers in the model')\n    parser.add_argument('--hidden_dim', type=int, default=128, help='Hidden dimension size')\n    parser.add_argument('--drop_out', type=float, default=0, help='Dropout rate')\n    parser.add_argument('--batch_sz', type=int, default=512, help='Batch size')\n    parser.add_argument('--learning_rate', type=float, default=0.01, help='Learning rate')\n    parser.add_argument('--output_path', type=str, default='./output', help='Path to save output')\n    \n    args = parser.parse_args()\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T07:20:11.327527Z","iopub.execute_input":"2025-04-16T07:20:11.327890Z","iopub.status.idle":"2025-04-16T07:20:11.334168Z","shell.execute_reply.started":"2025-04-16T07:20:11.327839Z","shell.execute_reply":"2025-04-16T07:20:11.333259Z"}},"outputs":[],"execution_count":32}]}