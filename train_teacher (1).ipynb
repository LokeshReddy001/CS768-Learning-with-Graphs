{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport networkx as nx\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n!pip install torch-geometric\nfrom torch_geometric.nn import GCNConv, SAGEConv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:32:22.259453Z","iopub.execute_input":"2025-04-10T08:32:22.259780Z","iopub.status.idle":"2025-04-10T08:32:25.562102Z","shell.execute_reply.started":"2025-04-10T08:32:22.259757Z","shell.execute_reply":"2025-04-10T08:32:25.561010Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.12)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.12.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2025.1.31)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch-geometric) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"TODO:\nImplement teacher network architecture and training along with flags for datasets and teacher model architecture (Chanikya and Nithin)\neg: python3 train_teacher.py --dataset=cora --model=SAGE --epochs-100 --lr=0.01 . Add flags for other hyperparameters if necessary (Chanikya and Nithin)\nOther teacher model architectures - GCN, GAT, APPNP (Chanikya and Nithin + others based on availability)","metadata":{}},{"cell_type":"code","source":"from torch_geometric.datasets import Planetoid\ndataset = Planetoid(root='./Cora', name='Cora')\n\ndata = dataset[0]\nprint(f'Dataset: {dataset}:')\nprint('======================')\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:32:25.563494Z","iopub.execute_input":"2025-04-10T08:32:25.563764Z","iopub.status.idle":"2025-04-10T08:32:25.586734Z","shell.execute_reply.started":"2025-04-10T08:32:25.563739Z","shell.execute_reply":"2025-04-10T08:32:25.585896Z"}},"outputs":[{"name":"stdout","text":"Dataset: Cora():\n======================\nNumber of graphs: 1\nNumber of features: 1433\nNumber of classes: 7\n","output_type":"stream"}],"execution_count":12},{"cell_type":"raw","source":"TODO: \n1. Standardise(lcc)\n2. binarize labels(dealing with multi labels)\n3. for now only worksfor cpf, extenf to ogb","metadata":{}},{"cell_type":"code","source":"# data.edge_index.t()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:32:25.588383Z","iopub.execute_input":"2025-04-10T08:32:25.588593Z","iopub.status.idle":"2025-04-10T08:32:25.591885Z","shell.execute_reply.started":"2025-04-10T08:32:25.588575Z","shell.execute_reply":"2025-04-10T08:32:25.591047Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def load_data(dataset):\n    if dataset == \"cora\":\n        dataset = Planetoid(root='./Cora', name='Cora')\n        data = dataset[0]\n        graph_nx = nx.Graph()\n        graph_nx.add_edges_from(data.edge_index.t().tolist())\n\n        # Adding self-loops\n        # graph_nx.add_edges_from((n, n) for n in graph_nx.nodes())\n        \n        # adj_tensor = torch.tensor(nx.to_numpy_array(graph_nx), dtype=torch.float).to('cuda')\n        features = data.x\n        labels = data.y\n\n        train_idx = data.train_mask.nonzero(as_tuple=True)[0]\n        val_idx = data.val_mask.nonzero(as_tuple=True)[0]\n        test_idx = data.test_mask.nonzero(as_tuple=True)[0]\n        \n        return data.edge_index, features, labels, data.train_mask, data.val_mask, data.test_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:32:25.593000Z","iopub.execute_input":"2025-04-10T08:32:25.593208Z","iopub.status.idle":"2025-04-10T08:32:25.606724Z","shell.execute_reply.started":"2025-04-10T08:32:25.593190Z","shell.execute_reply":"2025-04-10T08:32:25.606076Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"cnt = 0\n140+1000+500\n2708\nfor x in data.val_mask:\n    cnt += (x==True)\nprint(cnt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:32:25.607436Z","iopub.execute_input":"2025-04-10T08:32:25.607697Z","iopub.status.idle":"2025-04-10T08:32:25.647068Z","shell.execute_reply.started":"2025-04-10T08:32:25.607675Z","shell.execute_reply":"2025-04-10T08:32:25.646429Z"}},"outputs":[{"name":"stdout","text":"tensor(500)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"GCN => num layers, hidden, input dim, out, dp, activa,  ","metadata":{}},{"cell_type":"code","source":"class GCN(nn.Module):\n    def __init__(\n        self,\n        num_layers,\n        input_dim,\n        hidden_dim,\n        output_dim,\n        dropout_ratio,\n        activation,\n        norm_type=\"none\"\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.norm_type = norm_type\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.activation = activation\n\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        if num_layers == 1:\n            self.layers.append(GCNConv(input_dim, output_dim))\n        else:\n            self.layers.append(GCNConv(input_dim, hidden_dim))\n            if norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            for _ in range(num_layers - 2):\n                self.layers.append(GCNConv(hidden_dim, hidden_dim))\n                if norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            self.layers.append(GCNConv(hidden_dim, output_dim))\n\n    def forward(self, x, edge_index):\n        h_list = []\n        h = x\n        for l, layer in enumerate(self.layers):\n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n                h_list.append(h)\n        return h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:32:25.647774Z","iopub.execute_input":"2025-04-10T08:32:25.648049Z","iopub.status.idle":"2025-04-10T08:32:25.654879Z","shell.execute_reply.started":"2025-04-10T08:32:25.648014Z","shell.execute_reply":"2025-04-10T08:32:25.654019Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\n\nclass SAGE(nn.Module):\n    def __init__(\n        self,\n        num_layers,\n        input_dim,\n        hidden_dim,\n        output_dim,\n        dropout_ratio,\n        activation,\n        norm_type=\"none\",\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.norm_type = norm_type\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_ratio)\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n\n        if num_layers == 1:\n            self.layers.append(SAGEConv(input_dim, output_dim))\n        else:\n            self.layers.append(SAGEConv(input_dim, hidden_dim))\n            if self.norm_type == \"batch\":\n                self.norms.append(nn.BatchNorm1d(hidden_dim))\n            elif self.norm_type == \"layer\":\n                self.norms.append(nn.LayerNorm(hidden_dim))\n\n            for _ in range(num_layers - 2):\n                self.layers.append(SAGEConv(hidden_dim, hidden_dim))\n                if self.norm_type == \"batch\":\n                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n                elif self.norm_type == \"layer\":\n                    self.norms.append(nn.LayerNorm(hidden_dim))\n\n            self.layers.append(SAGEConv(hidden_dim, output_dim))\n\n    def forward(self, x, edge_index):\n        h = x\n        h_list = []\n        for l, layer in enumerate(self.layers):\n            h = layer(h, edge_index)\n            if l != self.num_layers - 1:\n                h_list.append(h)\n                if self.norm_type != \"none\":\n                    h = self.norms[l](h)\n                h = self.activation(h)\n                h = self.dropout(h)\n        return h\n\n\n    def inference(self, x_all, edge_index, batch_size=1024, device=\"cuda\"):\n        \"\"\"\n        Full-graph inference using mini-batches (for large graphs).\n        \"\"\"\n        from torch_geometric.loader import NeighborLoader\n\n        x = x_all.to(device)\n        for l, layer in enumerate(self.layers):\n            new_x = torch.zeros(\n                x_all.size(0),\n                self.hidden_dim if l != self.num_layers - 1 else self.output_dim,\n            ).to(device)\n\n            loader = NeighborLoader(\n                data=(x_all, edge_index),\n                input_nodes=torch.arange(x_all.size(0)),\n                num_neighbors=[-1],  # full neighbors\n                batch_size=batch_size,\n                shuffle=False\n            )\n\n            for batch in loader:\n                batch = batch.to(device)\n                h = x[batch.n_id]\n                h = layer(h, batch.edge_index)\n\n                if l != self.num_layers - 1:\n                    if self.norm_type != \"none\":\n                        h = self.norms[l](h)\n                    h = self.activation(h)\n                    h = self.dropout(h)\n\n                new_x[batch.n_id[:batch.batch_size]] = h\n\n            x = new_x\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:44:28.916048Z","iopub.execute_input":"2025-04-10T08:44:28.916354Z","iopub.status.idle":"2025-04-10T08:44:28.926890Z","shell.execute_reply.started":"2025-04-10T08:44:28.916330Z","shell.execute_reply":"2025-04-10T08:44:28.926135Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# For small, medium datasets few thousands, use model() in eval\n# For large ones like 100k or millions, use model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:44:30.371180Z","iopub.execute_input":"2025-04-10T08:44:30.371459Z","iopub.status.idle":"2025-04-10T08:44:30.374891Z","shell.execute_reply.started":"2025-04-10T08:44:30.371437Z","shell.execute_reply":"2025-04-10T08:44:30.373973Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def train_sage(model, loader, optimizer, criterion, device, homo=True):\n    model.train()\n    total_loss = 0\n\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        x = batch.x\n        y = batch.y[:batch.batch_size]  # Only use input nodes\n\n        if homo:\n            edge_index = batch.edge_index\n        else:\n            rel = list(batch.edge_index_dict.keys())[0]\n            edge_index = batch.edge_index_dict[rel]\n\n        out = model(x, edge_index)\n        out = out[:batch.batch_size]  # Only use predictions for input nodes\n\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\n\n@torch.no_grad()\ndef evaluate_sage(model, loader, device, homo=True):\n    model.eval()\n    correct = 0\n    total = 0\n\n    for batch in loader:\n        batch = batch.to(device)\n        x = batch.x\n        y = batch.y[:batch.batch_size]  # Only input nodes\n\n        if homo:\n            edge_index = batch.edge_index\n        else:\n            rel = list(batch.edge_index_dict.keys())[0]\n            edge_index = batch.edge_index_dict[rel]\n\n        out = model(x, edge_index)\n        out = out[:batch.batch_size]  # Only predictions for input nodes\n\n        pred = out.argmax(dim=1)\n        correct += (pred == y).sum().item()\n        total += y.size(0)\n\n    return correct / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:49:58.545583Z","iopub.execute_input":"2025-04-10T08:49:58.545883Z","iopub.status.idle":"2025-04-10T08:49:58.552998Z","shell.execute_reply.started":"2025-04-10T08:49:58.545861Z","shell.execute_reply":"2025-04-10T08:49:58.552153Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"from torch_geometric.loader import NeighborLoader\n\ntrain_loader = NeighborLoader(\n    data,\n    input_nodes=data.train_mask,\n    num_neighbors=[15, 10],\n    batch_size=128,\n    shuffle=True\n)\n\nval_loader = NeighborLoader(\n    data,\n    input_nodes=data.val_mask,\n    num_neighbors=[-1, -1],\n    batch_size=128\n)\n\ntest_loader = NeighborLoader(\n    data,\n    input_nodes=data.test_mask,\n    num_neighbors=[-1, -1],\n    batch_size=128\n)\n\n\nmodel = SAGE(\n    num_layers=2,\n    input_dim=dataset.num_node_features,\n    hidden_dim=128,\n    output_dim=dataset.num_classes,\n    dropout_ratio=0,\n    activation=nn.functional.relu,\n    norm_type=\"batch\"\n)\n\ndevice = 'cuda'\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\ncriterion = torch.nn.CrossEntropyLoss()\n\nfor epoch in range(1, 201):\n    loss = train_sage(model, train_loader, optimizer, criterion, device)\n    val_acc = evaluate_sage(model, val_loader, device)\n    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f}\")\n\ntest_acc = evaluate_sage(model, test_loader, device)\nprint(f\"Test Acc: {test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:56:53.661087Z","iopub.execute_input":"2025-04-10T08:56:53.661441Z","iopub.status.idle":"2025-04-10T08:57:07.246046Z","shell.execute_reply.started":"2025-04-10T08:56:53.661411Z","shell.execute_reply":"2025-04-10T08:57:07.245254Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 | Loss: 1.9665 | Val Acc: 0.3540\nEpoch 02 | Loss: 0.6244 | Val Acc: 0.4660\nEpoch 03 | Loss: 0.2881 | Val Acc: 0.5620\nEpoch 04 | Loss: 0.1405 | Val Acc: 0.6460\nEpoch 05 | Loss: 0.0713 | Val Acc: 0.7040\nEpoch 06 | Loss: 0.0565 | Val Acc: 0.7260\nEpoch 07 | Loss: 0.0291 | Val Acc: 0.7340\nEpoch 08 | Loss: 0.0259 | Val Acc: 0.7460\nEpoch 09 | Loss: 0.0148 | Val Acc: 0.7480\nEpoch 10 | Loss: 0.0132 | Val Acc: 0.7460\nEpoch 11 | Loss: 0.0103 | Val Acc: 0.7480\nEpoch 12 | Loss: 0.0062 | Val Acc: 0.7460\nEpoch 13 | Loss: 0.0080 | Val Acc: 0.7380\nEpoch 14 | Loss: 0.0061 | Val Acc: 0.7420\nEpoch 15 | Loss: 0.0049 | Val Acc: 0.7400\nEpoch 16 | Loss: 0.0032 | Val Acc: 0.7460\nEpoch 17 | Loss: 0.0026 | Val Acc: 0.7440\nEpoch 18 | Loss: 0.0036 | Val Acc: 0.7440\nEpoch 19 | Loss: 0.0028 | Val Acc: 0.7440\nEpoch 20 | Loss: 0.0023 | Val Acc: 0.7440\nEpoch 21 | Loss: 0.0023 | Val Acc: 0.7440\nEpoch 22 | Loss: 0.0029 | Val Acc: 0.7460\nEpoch 23 | Loss: 0.0026 | Val Acc: 0.7460\nEpoch 24 | Loss: 0.0025 | Val Acc: 0.7440\nEpoch 25 | Loss: 0.0020 | Val Acc: 0.7460\nEpoch 26 | Loss: 0.0018 | Val Acc: 0.7460\nEpoch 27 | Loss: 0.0018 | Val Acc: 0.7460\nEpoch 28 | Loss: 0.0017 | Val Acc: 0.7440\nEpoch 29 | Loss: 0.0016 | Val Acc: 0.7440\nEpoch 30 | Loss: 0.0022 | Val Acc: 0.7420\nEpoch 31 | Loss: 0.0013 | Val Acc: 0.7440\nEpoch 32 | Loss: 0.0015 | Val Acc: 0.7440\nEpoch 33 | Loss: 0.0017 | Val Acc: 0.7420\nEpoch 34 | Loss: 0.0018 | Val Acc: 0.7420\nEpoch 35 | Loss: 0.0014 | Val Acc: 0.7440\nEpoch 36 | Loss: 0.0019 | Val Acc: 0.7420\nEpoch 37 | Loss: 0.0017 | Val Acc: 0.7440\nEpoch 38 | Loss: 0.0015 | Val Acc: 0.7400\nEpoch 39 | Loss: 0.0012 | Val Acc: 0.7420\nEpoch 40 | Loss: 0.0013 | Val Acc: 0.7420\nEpoch 41 | Loss: 0.0013 | Val Acc: 0.7360\nEpoch 42 | Loss: 0.0012 | Val Acc: 0.7360\nEpoch 43 | Loss: 0.0014 | Val Acc: 0.7400\nEpoch 44 | Loss: 0.0016 | Val Acc: 0.7400\nEpoch 45 | Loss: 0.0011 | Val Acc: 0.7400\nEpoch 46 | Loss: 0.0013 | Val Acc: 0.7360\nEpoch 47 | Loss: 0.0013 | Val Acc: 0.7340\nEpoch 48 | Loss: 0.0012 | Val Acc: 0.7340\nEpoch 49 | Loss: 0.0013 | Val Acc: 0.7360\nEpoch 50 | Loss: 0.0009 | Val Acc: 0.7360\nEpoch 51 | Loss: 0.0009 | Val Acc: 0.7360\nEpoch 52 | Loss: 0.0008 | Val Acc: 0.7360\nEpoch 53 | Loss: 0.0012 | Val Acc: 0.7380\nEpoch 54 | Loss: 0.0010 | Val Acc: 0.7420\nEpoch 55 | Loss: 0.0011 | Val Acc: 0.7400\nEpoch 56 | Loss: 0.0010 | Val Acc: 0.7400\nEpoch 57 | Loss: 0.0009 | Val Acc: 0.7380\nEpoch 58 | Loss: 0.0010 | Val Acc: 0.7380\nEpoch 59 | Loss: 0.0009 | Val Acc: 0.7380\nEpoch 60 | Loss: 0.0011 | Val Acc: 0.7380\nEpoch 61 | Loss: 0.0010 | Val Acc: 0.7380\nEpoch 62 | Loss: 0.0012 | Val Acc: 0.7400\nEpoch 63 | Loss: 0.0008 | Val Acc: 0.7380\nEpoch 64 | Loss: 0.0010 | Val Acc: 0.7380\nEpoch 65 | Loss: 0.0009 | Val Acc: 0.7380\nEpoch 66 | Loss: 0.0011 | Val Acc: 0.7380\nEpoch 67 | Loss: 0.0008 | Val Acc: 0.7400\nEpoch 68 | Loss: 0.0011 | Val Acc: 0.7400\nEpoch 69 | Loss: 0.0010 | Val Acc: 0.7360\nEpoch 70 | Loss: 0.0008 | Val Acc: 0.7380\nEpoch 71 | Loss: 0.0016 | Val Acc: 0.7300\nEpoch 72 | Loss: 0.0011 | Val Acc: 0.7260\nEpoch 73 | Loss: 0.0008 | Val Acc: 0.7320\nEpoch 74 | Loss: 0.0009 | Val Acc: 0.7400\nEpoch 75 | Loss: 0.0011 | Val Acc: 0.7380\nEpoch 76 | Loss: 0.0008 | Val Acc: 0.7380\nEpoch 77 | Loss: 0.0010 | Val Acc: 0.7380\nEpoch 78 | Loss: 0.0017 | Val Acc: 0.7380\nEpoch 79 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 80 | Loss: 0.0008 | Val Acc: 0.7400\nEpoch 81 | Loss: 0.0008 | Val Acc: 0.7380\nEpoch 82 | Loss: 0.0008 | Val Acc: 0.7400\nEpoch 83 | Loss: 0.0009 | Val Acc: 0.7440\nEpoch 84 | Loss: 0.0011 | Val Acc: 0.7420\nEpoch 85 | Loss: 0.0008 | Val Acc: 0.7420\nEpoch 86 | Loss: 0.0008 | Val Acc: 0.7420\nEpoch 87 | Loss: 0.0009 | Val Acc: 0.7400\nEpoch 88 | Loss: 0.0007 | Val Acc: 0.7360\nEpoch 89 | Loss: 0.0009 | Val Acc: 0.7380\nEpoch 90 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 91 | Loss: 0.0008 | Val Acc: 0.7320\nEpoch 92 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 93 | Loss: 0.0011 | Val Acc: 0.7360\nEpoch 94 | Loss: 0.0007 | Val Acc: 0.7420\nEpoch 95 | Loss: 0.0009 | Val Acc: 0.7380\nEpoch 96 | Loss: 0.0009 | Val Acc: 0.7420\nEpoch 97 | Loss: 0.0008 | Val Acc: 0.7400\nEpoch 98 | Loss: 0.0008 | Val Acc: 0.7460\nEpoch 99 | Loss: 0.0009 | Val Acc: 0.7480\nEpoch 100 | Loss: 0.0007 | Val Acc: 0.7460\nEpoch 101 | Loss: 0.0007 | Val Acc: 0.7420\nEpoch 102 | Loss: 0.0008 | Val Acc: 0.7400\nEpoch 103 | Loss: 0.0008 | Val Acc: 0.7400\nEpoch 104 | Loss: 0.0009 | Val Acc: 0.7380\nEpoch 105 | Loss: 0.0007 | Val Acc: 0.7380\nEpoch 106 | Loss: 0.0007 | Val Acc: 0.7380\nEpoch 107 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 108 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 109 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 110 | Loss: 0.0008 | Val Acc: 0.7360\nEpoch 111 | Loss: 0.0007 | Val Acc: 0.7380\nEpoch 112 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 113 | Loss: 0.0008 | Val Acc: 0.7360\nEpoch 114 | Loss: 0.0007 | Val Acc: 0.7340\nEpoch 115 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 116 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 117 | Loss: 0.0008 | Val Acc: 0.7440\nEpoch 118 | Loss: 0.0008 | Val Acc: 0.7400\nEpoch 119 | Loss: 0.0007 | Val Acc: 0.7360\nEpoch 120 | Loss: 0.0007 | Val Acc: 0.7380\nEpoch 121 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 122 | Loss: 0.0008 | Val Acc: 0.7340\nEpoch 123 | Loss: 0.0010 | Val Acc: 0.7340\nEpoch 124 | Loss: 0.0006 | Val Acc: 0.7300\nEpoch 125 | Loss: 0.0008 | Val Acc: 0.7300\nEpoch 126 | Loss: 0.0006 | Val Acc: 0.7300\nEpoch 127 | Loss: 0.0005 | Val Acc: 0.7320\nEpoch 128 | Loss: 0.0011 | Val Acc: 0.7380\nEpoch 129 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 130 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 131 | Loss: 0.0007 | Val Acc: 0.7380\nEpoch 132 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 133 | Loss: 0.0007 | Val Acc: 0.7340\nEpoch 134 | Loss: 0.0007 | Val Acc: 0.7320\nEpoch 135 | Loss: 0.0008 | Val Acc: 0.7320\nEpoch 136 | Loss: 0.0008 | Val Acc: 0.7320\nEpoch 137 | Loss: 0.0008 | Val Acc: 0.7320\nEpoch 138 | Loss: 0.0007 | Val Acc: 0.7360\nEpoch 139 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 140 | Loss: 0.0007 | Val Acc: 0.7320\nEpoch 141 | Loss: 0.0005 | Val Acc: 0.7320\nEpoch 142 | Loss: 0.0007 | Val Acc: 0.7320\nEpoch 143 | Loss: 0.0007 | Val Acc: 0.7300\nEpoch 144 | Loss: 0.0008 | Val Acc: 0.7400\nEpoch 145 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 146 | Loss: 0.0009 | Val Acc: 0.7440\nEpoch 147 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 148 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 149 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 150 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 151 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 152 | Loss: 0.0005 | Val Acc: 0.7340\nEpoch 153 | Loss: 0.0006 | Val Acc: 0.7320\nEpoch 154 | Loss: 0.0010 | Val Acc: 0.7340\nEpoch 155 | Loss: 0.0007 | Val Acc: 0.7380\nEpoch 156 | Loss: 0.0007 | Val Acc: 0.7360\nEpoch 157 | Loss: 0.0006 | Val Acc: 0.7420\nEpoch 158 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 159 | Loss: 0.0007 | Val Acc: 0.7460\nEpoch 160 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 161 | Loss: 0.0006 | Val Acc: 0.7420\nEpoch 162 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 163 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 164 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 165 | Loss: 0.0008 | Val Acc: 0.7340\nEpoch 166 | Loss: 0.0008 | Val Acc: 0.7380\nEpoch 167 | Loss: 0.0006 | Val Acc: 0.7420\nEpoch 168 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 169 | Loss: 0.0005 | Val Acc: 0.7340\nEpoch 170 | Loss: 0.0009 | Val Acc: 0.7340\nEpoch 171 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 172 | Loss: 0.0007 | Val Acc: 0.7440\nEpoch 173 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 174 | Loss: 0.0006 | Val Acc: 0.7440\nEpoch 175 | Loss: 0.0006 | Val Acc: 0.7420\nEpoch 176 | Loss: 0.0006 | Val Acc: 0.7440\nEpoch 177 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 178 | Loss: 0.0008 | Val Acc: 0.7420\nEpoch 179 | Loss: 0.0006 | Val Acc: 0.7420\nEpoch 180 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 181 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 182 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 183 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 184 | Loss: 0.0005 | Val Acc: 0.7340\nEpoch 185 | Loss: 0.0008 | Val Acc: 0.7340\nEpoch 186 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 187 | Loss: 0.0006 | Val Acc: 0.7340\nEpoch 188 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 189 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 190 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 191 | Loss: 0.0008 | Val Acc: 0.7320\nEpoch 192 | Loss: 0.0007 | Val Acc: 0.7320\nEpoch 193 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 194 | Loss: 0.0004 | Val Acc: 0.7360\nEpoch 195 | Loss: 0.0005 | Val Acc: 0.7340\nEpoch 196 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 197 | Loss: 0.0007 | Val Acc: 0.7360\nEpoch 198 | Loss: 0.0005 | Val Acc: 0.7340\nEpoch 199 | Loss: 0.0007 | Val Acc: 0.7300\nEpoch 200 | Loss: 0.0005 | Val Acc: 0.7340\nEpoch 201 | Loss: 0.0005 | Val Acc: 0.7280\nEpoch 202 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 203 | Loss: 0.0007 | Val Acc: 0.7460\nEpoch 204 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 205 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 206 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 207 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 208 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 209 | Loss: 0.0006 | Val Acc: 0.7420\nEpoch 210 | Loss: 0.0006 | Val Acc: 0.7420\nEpoch 211 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 212 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 213 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 214 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 215 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 216 | Loss: 0.0005 | Val Acc: 0.7300\nEpoch 217 | Loss: 0.0005 | Val Acc: 0.7320\nEpoch 218 | Loss: 0.0005 | Val Acc: 0.7320\nEpoch 219 | Loss: 0.0005 | Val Acc: 0.7340\nEpoch 220 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 221 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 222 | Loss: 0.0006 | Val Acc: 0.7340\nEpoch 223 | Loss: 0.0006 | Val Acc: 0.7300\nEpoch 224 | Loss: 0.0005 | Val Acc: 0.7340\nEpoch 225 | Loss: 0.0004 | Val Acc: 0.7320\nEpoch 226 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 227 | Loss: 0.0006 | Val Acc: 0.7340\nEpoch 228 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 229 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 230 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 231 | Loss: 0.0006 | Val Acc: 0.7320\nEpoch 232 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 233 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 234 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 235 | Loss: 0.0006 | Val Acc: 0.7460\nEpoch 236 | Loss: 0.0006 | Val Acc: 0.7440\nEpoch 237 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 238 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 239 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 240 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 241 | Loss: 0.0008 | Val Acc: 0.7380\nEpoch 242 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 243 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 244 | Loss: 0.0007 | Val Acc: 0.7360\nEpoch 245 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 246 | Loss: 0.0004 | Val Acc: 0.7340\nEpoch 247 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 248 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 249 | Loss: 0.0008 | Val Acc: 0.7360\nEpoch 250 | Loss: 0.0004 | Val Acc: 0.7360\nEpoch 251 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 252 | Loss: 0.0007 | Val Acc: 0.7320\nEpoch 253 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 254 | Loss: 0.0005 | Val Acc: 0.7340\nEpoch 255 | Loss: 0.0005 | Val Acc: 0.7300\nEpoch 256 | Loss: 0.0005 | Val Acc: 0.7280\nEpoch 257 | Loss: 0.0006 | Val Acc: 0.7340\nEpoch 258 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 259 | Loss: 0.0004 | Val Acc: 0.7360\nEpoch 260 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 261 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 262 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 263 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 264 | Loss: 0.0004 | Val Acc: 0.7420\nEpoch 265 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 266 | Loss: 0.0004 | Val Acc: 0.7400\nEpoch 267 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 268 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 269 | Loss: 0.0008 | Val Acc: 0.7360\nEpoch 270 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 271 | Loss: 0.0014 | Val Acc: 0.7360\nEpoch 272 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 273 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 274 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 275 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 276 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 277 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 278 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 279 | Loss: 0.0006 | Val Acc: 0.7440\nEpoch 280 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 281 | Loss: 0.0004 | Val Acc: 0.7400\nEpoch 282 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 283 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 284 | Loss: 0.0007 | Val Acc: 0.7400\nEpoch 285 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 286 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 287 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 288 | Loss: 0.0004 | Val Acc: 0.7340\nEpoch 289 | Loss: 0.0004 | Val Acc: 0.7360\nEpoch 290 | Loss: 0.0004 | Val Acc: 0.7360\nEpoch 291 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 292 | Loss: 0.0008 | Val Acc: 0.7380\nEpoch 293 | Loss: 0.0005 | Val Acc: 0.7320\nEpoch 294 | Loss: 0.0004 | Val Acc: 0.7320\nEpoch 295 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 296 | Loss: 0.0004 | Val Acc: 0.7400\nEpoch 297 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 298 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 299 | Loss: 0.0004 | Val Acc: 0.7420\nEpoch 300 | Loss: 0.0007 | Val Acc: 0.7460\nEpoch 301 | Loss: 0.0005 | Val Acc: 0.7480\nEpoch 302 | Loss: 0.0006 | Val Acc: 0.7440\nEpoch 303 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 304 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 305 | Loss: 0.0004 | Val Acc: 0.7400\nEpoch 306 | Loss: 0.0005 | Val Acc: 0.7340\nEpoch 307 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 308 | Loss: 0.0004 | Val Acc: 0.7340\nEpoch 309 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 310 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 311 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 312 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 313 | Loss: 0.0005 | Val Acc: 0.7400\nEpoch 314 | Loss: 0.0004 | Val Acc: 0.7360\nEpoch 315 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 316 | Loss: 0.0004 | Val Acc: 0.7440\nEpoch 317 | Loss: 0.0006 | Val Acc: 0.7420\nEpoch 318 | Loss: 0.0006 | Val Acc: 0.7440\nEpoch 319 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 320 | Loss: 0.0007 | Val Acc: 0.7440\nEpoch 321 | Loss: 0.0006 | Val Acc: 0.7460\nEpoch 322 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 323 | Loss: 0.0013 | Val Acc: 0.7440\nEpoch 324 | Loss: 0.0007 | Val Acc: 0.7440\nEpoch 325 | Loss: 0.0004 | Val Acc: 0.7460\nEpoch 326 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 327 | Loss: 0.0006 | Val Acc: 0.7440\nEpoch 328 | Loss: 0.0009 | Val Acc: 0.7400\nEpoch 329 | Loss: 0.0008 | Val Acc: 0.7440\nEpoch 330 | Loss: 0.0006 | Val Acc: 0.7440\nEpoch 331 | Loss: 0.0004 | Val Acc: 0.7440\nEpoch 332 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 333 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 334 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 335 | Loss: 0.0004 | Val Acc: 0.7440\nEpoch 336 | Loss: 0.0004 | Val Acc: 0.7440\nEpoch 337 | Loss: 0.0004 | Val Acc: 0.7400\nEpoch 338 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 339 | Loss: 0.0005 | Val Acc: 0.7320\nEpoch 340 | Loss: 0.0004 | Val Acc: 0.7320\nEpoch 341 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 342 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 343 | Loss: 0.0005 | Val Acc: 0.7320\nEpoch 344 | Loss: 0.0006 | Val Acc: 0.7360\nEpoch 345 | Loss: 0.0004 | Val Acc: 0.7360\nEpoch 346 | Loss: 0.0004 | Val Acc: 0.7340\nEpoch 347 | Loss: 0.0004 | Val Acc: 0.7280\nEpoch 348 | Loss: 0.0004 | Val Acc: 0.7260\nEpoch 349 | Loss: 0.0004 | Val Acc: 0.7300\nEpoch 350 | Loss: 0.0004 | Val Acc: 0.7340\nEpoch 351 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 352 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 353 | Loss: 0.0005 | Val Acc: 0.7360\nEpoch 354 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 355 | Loss: 0.0004 | Val Acc: 0.7400\nEpoch 356 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 357 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 358 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 359 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 360 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 361 | Loss: 0.0004 | Val Acc: 0.7380\nEpoch 362 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 363 | Loss: 0.0007 | Val Acc: 0.7440\nEpoch 364 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 365 | Loss: 0.0004 | Val Acc: 0.7420\nEpoch 366 | Loss: 0.0007 | Val Acc: 0.7440\nEpoch 367 | Loss: 0.0005 | Val Acc: 0.7480\nEpoch 368 | Loss: 0.0005 | Val Acc: 0.7500\nEpoch 369 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 370 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 371 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 372 | Loss: 0.0005 | Val Acc: 0.7520\nEpoch 373 | Loss: 0.0005 | Val Acc: 0.7500\nEpoch 374 | Loss: 0.0004 | Val Acc: 0.7440\nEpoch 375 | Loss: 0.0006 | Val Acc: 0.7460\nEpoch 376 | Loss: 0.0004 | Val Acc: 0.7460\nEpoch 377 | Loss: 0.0004 | Val Acc: 0.7460\nEpoch 378 | Loss: 0.0005 | Val Acc: 0.7520\nEpoch 379 | Loss: 0.0004 | Val Acc: 0.7500\nEpoch 380 | Loss: 0.0004 | Val Acc: 0.7480\nEpoch 381 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 382 | Loss: 0.0004 | Val Acc: 0.7400\nEpoch 383 | Loss: 0.0008 | Val Acc: 0.7340\nEpoch 384 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 385 | Loss: 0.0004 | Val Acc: 0.7440\nEpoch 386 | Loss: 0.0006 | Val Acc: 0.7440\nEpoch 387 | Loss: 0.0004 | Val Acc: 0.7440\nEpoch 388 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 389 | Loss: 0.0007 | Val Acc: 0.7480\nEpoch 390 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 391 | Loss: 0.0006 | Val Acc: 0.7400\nEpoch 392 | Loss: 0.0006 | Val Acc: 0.7420\nEpoch 393 | Loss: 0.0004 | Val Acc: 0.7400\nEpoch 394 | Loss: 0.0005 | Val Acc: 0.7380\nEpoch 395 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 396 | Loss: 0.0009 | Val Acc: 0.7420\nEpoch 397 | Loss: 0.0007 | Val Acc: 0.7500\nEpoch 398 | Loss: 0.0004 | Val Acc: 0.7520\nEpoch 399 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 400 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 401 | Loss: 0.0005 | Val Acc: 0.7500\nEpoch 402 | Loss: 0.0006 | Val Acc: 0.7460\nEpoch 403 | Loss: 0.0004 | Val Acc: 0.7500\nEpoch 404 | Loss: 0.0006 | Val Acc: 0.7540\nEpoch 405 | Loss: 0.0005 | Val Acc: 0.7500\nEpoch 406 | Loss: 0.0005 | Val Acc: 0.7500\nEpoch 407 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 408 | Loss: 0.0005 | Val Acc: 0.7540\nEpoch 409 | Loss: 0.0005 | Val Acc: 0.7520\nEpoch 410 | Loss: 0.0005 | Val Acc: 0.7520\nEpoch 411 | Loss: 0.0007 | Val Acc: 0.7540\nEpoch 412 | Loss: 0.0006 | Val Acc: 0.7520\nEpoch 413 | Loss: 0.0004 | Val Acc: 0.7520\nEpoch 414 | Loss: 0.0006 | Val Acc: 0.7440\nEpoch 415 | Loss: 0.0004 | Val Acc: 0.7500\nEpoch 416 | Loss: 0.0004 | Val Acc: 0.7460\nEpoch 417 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 418 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 419 | Loss: 0.0004 | Val Acc: 0.7460\nEpoch 420 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 421 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 422 | Loss: 0.0004 | Val Acc: 0.7480\nEpoch 423 | Loss: 0.0004 | Val Acc: 0.7500\nEpoch 424 | Loss: 0.0005 | Val Acc: 0.7500\nEpoch 425 | Loss: 0.0004 | Val Acc: 0.7460\nEpoch 426 | Loss: 0.0004 | Val Acc: 0.7480\nEpoch 427 | Loss: 0.0005 | Val Acc: 0.7500\nEpoch 428 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 429 | Loss: 0.0005 | Val Acc: 0.7480\nEpoch 430 | Loss: 0.0011 | Val Acc: 0.7540\nEpoch 431 | Loss: 0.0004 | Val Acc: 0.7520\nEpoch 432 | Loss: 0.0007 | Val Acc: 0.7540\nEpoch 433 | Loss: 0.0005 | Val Acc: 0.7460\nEpoch 434 | Loss: 0.0004 | Val Acc: 0.7460\nEpoch 435 | Loss: 0.0003 | Val Acc: 0.7440\nEpoch 436 | Loss: 0.0010 | Val Acc: 0.7380\nEpoch 437 | Loss: 0.0004 | Val Acc: 0.7400\nEpoch 438 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 439 | Loss: 0.0005 | Val Acc: 0.7420\nEpoch 440 | Loss: 0.0006 | Val Acc: 0.7380\nEpoch 441 | Loss: 0.0006 | Val Acc: 0.7500\nEpoch 442 | Loss: 0.0007 | Val Acc: 0.7500\nEpoch 443 | Loss: 0.0004 | Val Acc: 0.7500\nEpoch 444 | Loss: 0.0005 | Val Acc: 0.7500\nEpoch 445 | Loss: 0.0004 | Val Acc: 0.7500\nEpoch 446 | Loss: 0.0005 | Val Acc: 0.7500\nEpoch 447 | Loss: 0.0004 | Val Acc: 0.7460\nEpoch 448 | Loss: 0.0004 | Val Acc: 0.7480\nEpoch 449 | Loss: 0.0005 | Val Acc: 0.7520\nEpoch 450 | Loss: 0.0011 | Val Acc: 0.7520\nEpoch 451 | Loss: 0.0005 | Val Acc: 0.7440\nEpoch 452 | Loss: 0.0004 | Val Acc: 0.7460\nEpoch 453 | Loss: 0.0004 | Val Acc: 0.7480\nEpoch 454 | Loss: 0.0005 | Val Acc: 0.7480\nEpoch 455 | Loss: 0.0004 | Val Acc: 0.7540\nEpoch 456 | Loss: 0.0004 | Val Acc: 0.7480\nEpoch 457 | Loss: 0.0004 | Val Acc: 0.7520\nEpoch 458 | Loss: 0.0005 | Val Acc: 0.7540\nEpoch 459 | Loss: 0.0006 | Val Acc: 0.7500\nEpoch 460 | Loss: 0.0004 | Val Acc: 0.7500\nEpoch 461 | Loss: 0.0005 | Val Acc: 0.7560\nEpoch 462 | Loss: 0.0005 | Val Acc: 0.7540\nEpoch 463 | Loss: 0.0005 | Val Acc: 0.7540\nEpoch 464 | Loss: 0.0006 | Val Acc: 0.7580\nEpoch 465 | Loss: 0.0007 | Val Acc: 0.7560\nEpoch 466 | Loss: 0.0005 | Val Acc: 0.7540\nEpoch 467 | Loss: 0.0004 | Val Acc: 0.7500\nEpoch 468 | Loss: 0.0006 | Val Acc: 0.7520\nEpoch 469 | Loss: 0.0005 | Val Acc: 0.7540\nEpoch 470 | Loss: 0.0006 | Val Acc: 0.7520\nEpoch 471 | Loss: 0.0007 | Val Acc: 0.7560\nEpoch 472 | Loss: 0.0004 | Val Acc: 0.7560\nEpoch 473 | Loss: 0.0004 | Val Acc: 0.7520\nEpoch 474 | Loss: 0.0005 | Val Acc: 0.7480\nEpoch 475 | Loss: 0.0004 | Val Acc: 0.7500\nEpoch 476 | Loss: 0.0004 | Val Acc: 0.7480\nEpoch 477 | Loss: 0.0005 | Val Acc: 0.7520\nEpoch 478 | Loss: 0.0005 | Val Acc: 0.7480\nEpoch 479 | Loss: 0.0009 | Val Acc: 0.7520\nEpoch 480 | Loss: 0.0004 | Val Acc: 0.7540\nEpoch 481 | Loss: 0.0005 | Val Acc: 0.7560\nEpoch 482 | Loss: 0.0004 | Val Acc: 0.7600\nEpoch 483 | Loss: 0.0006 | Val Acc: 0.7580\nEpoch 484 | Loss: 0.0005 | Val Acc: 0.7560\nEpoch 485 | Loss: 0.0005 | Val Acc: 0.7560\nEpoch 486 | Loss: 0.0005 | Val Acc: 0.7540\nEpoch 487 | Loss: 0.0004 | Val Acc: 0.7520\nEpoch 488 | Loss: 0.0005 | Val Acc: 0.7540\nEpoch 489 | Loss: 0.0004 | Val Acc: 0.7540\nEpoch 490 | Loss: 0.0004 | Val Acc: 0.7560\nEpoch 491 | Loss: 0.0004 | Val Acc: 0.7540\nEpoch 492 | Loss: 0.0005 | Val Acc: 0.7520\nEpoch 493 | Loss: 0.0006 | Val Acc: 0.7520\nEpoch 494 | Loss: 0.0005 | Val Acc: 0.7540\nEpoch 495 | Loss: 0.0005 | Val Acc: 0.7540\nEpoch 496 | Loss: 0.0008 | Val Acc: 0.7520\nEpoch 497 | Loss: 0.0005 | Val Acc: 0.7580\nEpoch 498 | Loss: 0.0006 | Val Acc: 0.7560\nEpoch 499 | Loss: 0.0005 | Val Acc: 0.7560\nEpoch 500 | Loss: 0.0004 | Val Acc: 0.7560\nTest Acc: 0.7840\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"def train(model, data, edge_index, labels, train_idx, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data, edge_index)\n    loss = criterion(out[train_idx], labels[train_idx])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n@torch.no_grad()\ndef evaluate(model, data, edge_index, labels, idx):\n    model.eval()\n    out = model(data, edge_index)\n    pred = out[idx].argmax(dim=1)\n    correct = (pred == labels[idx]).sum().item()\n    acc = correct / sum(idx)\n    return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:52:29.031834Z","iopub.execute_input":"2025-04-10T08:52:29.032175Z","iopub.status.idle":"2025-04-10T08:52:29.037561Z","shell.execute_reply.started":"2025-04-10T08:52:29.032148Z","shell.execute_reply":"2025-04-10T08:52:29.036529Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"edge_index, features, labels, train_idx, val_idx, test_idx = load_data(\"cora\")\nmodel = GCN(\n    num_layers=3,\n    input_dim=dataset.num_node_features,\n    hidden_dim=64,\n    output_dim=dataset.num_classes,\n    dropout_ratio=0.8,\n    activation=nn.functional.relu,\n    norm_type=\"batch\"\n)\n# model = GCN1(dataset.num_node_features, 64, dataset.num_classes)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\ncriterion = torch.nn.CrossEntropyLoss()\n\nfor epoch in range(1, 150):\n    loss = train(model, features, edge_index, labels, train_idx, optimizer, criterion)\n    val_acc = evaluate(model, features, edge_index, labels, val_idx)\n    if epoch % 10 == 0 or epoch == 1:\n        test_acc = evaluate(model, features, edge_index, labels, test_idx)\n        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:32:29.280835Z","iopub.status.idle":"2025-04-10T08:32:29.281244Z","shell.execute_reply":"2025-04-10T08:32:29.281070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model(data.x, data.edge_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:32:29.282262Z","iopub.status.idle":"2025-04-10T08:32:29.282638Z","shell.execute_reply":"2025-04-10T08:32:29.282477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Teacher:\n    def __init__(self, args):\n        self.args = args\n        pass\n    def graph_split():\n        pass\n    def train_transductive():\n        pass\n    def train_inductive():\n        pass\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:32:29.283508Z","iopub.status.idle":"2025-04-10T08:32:29.283882Z","shell.execute_reply":"2025-04-10T08:32:29.283719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    parser = argparse.ArgumentParser(description=\"Teacher implementation\")\n    parser.add_argument('--num_runs', type=int, default=1, help='Number of runs')\n    parser.add_argument('--setting', type=str, choices=['trans', 'ind'], required=True, help='Setting type: trans or ind')\n    parser.add_argument('--data_path', type=str, required=True, help='Path to the dataset')\n    parser.add_argument('--model_name', type=str, default='SAGE', help='Name of the model(SAGE, GCN, GAT, APPNP)')\n    parser.add_argument('--num_layers', type=int, default=2, help='Number of layers in the model')\n    parser.add_argument('--hidden_dim', type=int, default=128, help='Hidden dimension size')\n    parser.add_argument('--drop_out', type=float, default=0, help='Dropout rate')\n    parser.add_argument('--batch_sz', type=int, default=512, help='Batch size')\n    parser.add_argument('--learning_rate', type=float, default=0.01, help='Learning rate')\n    parser.add_argument('--output_path', type=str, default='./output', help='Path to save output')\n    \n    args = parser.parse_args()\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T08:32:29.284707Z","iopub.status.idle":"2025-04-10T08:32:29.285157Z","shell.execute_reply":"2025-04-10T08:32:29.284986Z"}},"outputs":[],"execution_count":null}]}