{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c46c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from deepwalk.deepwalk import DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4eeef75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers, input_feat_dim, position_emb_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_feat_dim = input_feat_dim\n",
    "        self.position_emb_dim = position_emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.rsd_encoder = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layers.append(nn.Linear(input_feat_dim + position_emb_dim, hidden_dim))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        for layer in self.layers[:-1]:\n",
    "            inp = F.relu(layer(inp))\n",
    "            inp = self.dropout(inp)\n",
    "        out = self.layers[-1](inp)\n",
    "        return inp, out\n",
    "    \n",
    "    def MLP_RSD(self, mlp_emb):\n",
    "        return F.relu(self.rsd_encoder(mlp_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1d61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='./Cora', name='Cora')\n",
    "\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeb6db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph_nx = nx.Graph()\n",
    "graph_nx.add_edges_from(data.edge_index.t().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88cd6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  2709\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  126582 lr:  0.000000 avg.loss:  3.680919 ETA:   0h 0m 0s\n",
      "/Users/plslokeshreddy/Documents/projects/lwg/CS768-Learning-with-Graphs/deepwalk/deepwalk.py:63: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  return torch.tensor(embeddings)\n"
     ]
    }
   ],
   "source": [
    "deepwalk_model = DeepWalk(graph_nx, walk_length=80, walks_per_vertex=10)\n",
    "deepwalk_model.train()\n",
    "position_embeddings = deepwalk_model.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2d89a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "487f4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(num_layers=3,\n",
    "          input_feat_dim=data.x.shape[1],\n",
    "          position_emb_dim=position_embeddings.shape[1],\n",
    "          hidden_dim=64,\n",
    "          output_dim=dataset.num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mlp = mlp.to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "71ebb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_emb = torch.load('teacher_outputs/embeddings.pt')\n",
    "teacher_out = torch.load('teacher_outputs/label_scores.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "41480021",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss_fn = nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8a29c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_delta(model, X, Y, epsilon=0.01, step_size=0.001, num_iterations=10):\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Forward pass with current perturbation\n",
    "        X_adv = X + delta\n",
    "        _, logits = model(X_adv)\n",
    "        \n",
    "        # Calculate loss (negative, since we're maximizing the loss for adversarial examples)\n",
    "        loss = F.cross_entropy(logits, Y)\n",
    "        \n",
    "        # Backward pass to get gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update delta using the sign of gradient (Eq. 6)\n",
    "        with torch.no_grad():\n",
    "            delta_grad = delta.grad.data\n",
    "            delta_update = step_size * torch.sign(delta_grad)\n",
    "            delta.data = delta.data + delta_update\n",
    "            \n",
    "            # Project delta back to epsilon-ball (L_p norm constraint)\n",
    "            if torch.norm(delta.data, p=2) > epsilon:\n",
    "                delta.data = epsilon * delta.data / torch.norm(delta.data, p=2)\n",
    "            \n",
    "        # Reset gradients for next iteration\n",
    "        if delta.grad is not None:\n",
    "            delta.grad.zero_()\n",
    "    \n",
    "    return delta.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "47d33f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Loss: 2.8204, Train Acc: 0.1429, Val Acc: 0.3160, Test Acc: 0.3210\n",
      "Epoch 010, Loss: 1.0083, Train Acc: 0.9429, Val Acc: 0.9060, Test Acc: 0.9080\n",
      "Epoch 020, Loss: 0.7623, Train Acc: 1.0000, Val Acc: 0.9860, Test Acc: 0.9920\n",
      "Epoch 030, Loss: 0.6638, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 0.9990\n",
      "Epoch 040, Loss: 0.6458, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 050, Loss: 0.6003, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 060, Loss: 0.5426, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 070, Loss: 0.5193, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 080, Loss: 0.5034, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 090, Loss: 0.4855, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 100, Loss: 0.4852, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 110, Loss: 0.4746, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 120, Loss: 0.4791, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 130, Loss: 0.5004, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 140, Loss: 0.4712, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 150, Loss: 0.4825, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 160, Loss: 0.4616, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 170, Loss: 0.4620, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 180, Loss: 0.4624, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 190, Loss: 0.4497, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch 200, Loss: 0.4588, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    mlp.train()\n",
    "    optimizer.zero_grad()\n",
    "    inp = torch.cat([data.x, position_embeddings.to(device)], dim=-1)  \n",
    "    mlp_emb, mlp_out = mlp(inp)\n",
    "\n",
    "    # GROUND TRUTH Cross Entropy Loss\n",
    "    GT_loss = F.cross_entropy(mlp_out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    # SOFT LABELS KL Divergence Loss\n",
    "    temp = 1\n",
    "    SL_Loss = kl_loss_fn(F.log_softmax(mlp_out/temp, dim=1), \n",
    "                         F.softmax(teacher_out/temp, dim=1)) * (temp**2)\n",
    "    \n",
    "    # Representational Similarity Distillation Loss\n",
    "    teacher_mat = teacher_emb @ teacher_emb.t()\n",
    "    encoded_mlp_meb = mlp.MLP_RSD(mlp_emb)\n",
    "    mlp_mat = encoded_mlp_meb @ encoded_mlp_meb.t()\n",
    "    RSD_Loss = torch.mean((mlp_mat - teacher_mat) ** 2)\n",
    "\n",
    "    # Adversarial Feature Augmentation Loss\n",
    "    inp = torch.cat([data.x, position_embeddings.to(device)], dim=-1)  \n",
    "    delta = pgd_delta(mlp, inp, data.y, epsilon=0.01, step_size=0.001, num_iterations=10)\n",
    "    _, mlp_out = mlp(inp + delta)\n",
    "    ADV_loss = F.cross_entropy(mlp_out[data.train_mask], data.y[data.train_mask]) + F.cross_entropy(mlp_out, F.softmax(teacher_out, dim=1))\n",
    "    \n",
    "    loss = 1.0*GT_loss + 0.5*SL_Loss + 0.0*RSD_Loss + 0.0*ADV_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    mlp.eval()\n",
    "    inp = torch.cat([data.x, position_embeddings.to(device)], dim=-1)  \n",
    "    _, out = mlp(inp)\n",
    "    pred = out.argmax(dim=1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask] == data.y[mask]\n",
    "        acc = int(correct.sum()) / int(mask.sum())\n",
    "        accs.append(acc)\n",
    "    return accs  # train_acc, val_acc, test_acc\n",
    "\n",
    "# Training for 200 epochs\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec4800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
