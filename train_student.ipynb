{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c46c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from deepwalk.deepwalk import DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eeef75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers, input_feat_dim, position_emb_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_feat_dim = input_feat_dim\n",
    "        self.position_emb_dim = position_emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(input_feat_dim + position_emb_dim, hidden_dim))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "    \n",
    "    def forward(self, x, position_emb):\n",
    "        inp = torch.cat([x, position_emb], dim=-1)  \n",
    "        for layer in self.layers[:-1]:\n",
    "            inp = F.relu(layer(inp))\n",
    "            inp = self.dropout(inp)\n",
    "        out = self.layers[-1](inp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1d61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='./Cora', name='Cora')\n",
    "\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb6db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph_nx = nx.Graph()\n",
    "graph_nx.add_edges_from(data.edge_index.t().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88cd6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  2709\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  128333 lr:  0.000000 avg.loss:  3.686964 ETA:   0h 0m 0s\n",
      "/Users/plslokeshreddy/Documents/projects/lwg/CS768-Learning-with-Graphs/deepwalk/deepwalk.py:63: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  return torch.tensor(embeddings)\n"
     ]
    }
   ],
   "source": [
    "deepwalk_model = DeepWalk(graph_nx, walk_length=80, walks_per_vertex=10)\n",
    "deepwalk_model.train()\n",
    "position_embeddings = deepwalk_model.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2d89a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "487f4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(num_layers=3,\n",
    "          input_feat_dim=data.x.shape[1],\n",
    "          position_emb_dim=position_embeddings.shape[1],\n",
    "          hidden_dim=64,\n",
    "          output_dim=dataset.num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mlp = mlp.to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d33f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Loss: 1.9534, Train Acc: 0.2214, Val Acc: 0.1300, Test Acc: 0.1390\n",
      "Epoch 010, Loss: 0.5619, Train Acc: 0.9929, Val Acc: 0.7240, Test Acc: 0.7060\n",
      "Epoch 020, Loss: 0.1274, Train Acc: 1.0000, Val Acc: 0.7260, Test Acc: 0.7180\n",
      "Epoch 030, Loss: 0.0295, Train Acc: 1.0000, Val Acc: 0.7340, Test Acc: 0.7100\n",
      "Epoch 040, Loss: 0.0241, Train Acc: 1.0000, Val Acc: 0.7000, Test Acc: 0.7050\n",
      "Epoch 050, Loss: 0.0780, Train Acc: 1.0000, Val Acc: 0.7220, Test Acc: 0.7340\n",
      "Epoch 060, Loss: 0.0649, Train Acc: 1.0000, Val Acc: 0.7200, Test Acc: 0.7120\n",
      "Epoch 070, Loss: 0.0611, Train Acc: 1.0000, Val Acc: 0.7180, Test Acc: 0.7220\n",
      "Epoch 080, Loss: 0.0242, Train Acc: 1.0000, Val Acc: 0.6760, Test Acc: 0.6700\n",
      "Epoch 090, Loss: 0.0438, Train Acc: 1.0000, Val Acc: 0.6960, Test Acc: 0.6960\n",
      "Epoch 100, Loss: 0.0211, Train Acc: 1.0000, Val Acc: 0.7100, Test Acc: 0.7020\n",
      "Epoch 110, Loss: 0.0174, Train Acc: 1.0000, Val Acc: 0.6920, Test Acc: 0.6920\n",
      "Epoch 120, Loss: 0.0406, Train Acc: 1.0000, Val Acc: 0.7120, Test Acc: 0.7060\n",
      "Epoch 130, Loss: 0.0067, Train Acc: 1.0000, Val Acc: 0.7300, Test Acc: 0.7140\n",
      "Epoch 140, Loss: 0.0035, Train Acc: 1.0000, Val Acc: 0.7180, Test Acc: 0.7200\n",
      "Epoch 150, Loss: 0.0202, Train Acc: 1.0000, Val Acc: 0.7020, Test Acc: 0.6940\n",
      "Epoch 160, Loss: 0.0191, Train Acc: 1.0000, Val Acc: 0.6960, Test Acc: 0.6970\n",
      "Epoch 170, Loss: 0.0226, Train Acc: 1.0000, Val Acc: 0.7020, Test Acc: 0.7040\n",
      "Epoch 180, Loss: 0.0144, Train Acc: 1.0000, Val Acc: 0.7060, Test Acc: 0.7050\n",
      "Epoch 190, Loss: 0.0069, Train Acc: 1.0000, Val Acc: 0.7080, Test Acc: 0.7040\n",
      "Epoch 200, Loss: 0.0114, Train Acc: 1.0000, Val Acc: 0.7140, Test Acc: 0.7180\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    mlp.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = mlp(data.x, position_embeddings.to(device))  # forward pass\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    mlp.eval()\n",
    "    out = mlp(data.x, position_embeddings.to(device))\n",
    "    pred = out.argmax(dim=1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask] == data.y[mask]\n",
    "        acc = int(correct.sum()) / int(mask.sum())\n",
    "        accs.append(acc)\n",
    "    return accs  # train_acc, val_acc, test_acc\n",
    "\n",
    "# Training for 200 epochs\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec4800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
