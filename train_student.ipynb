{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c46c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from deepwalk.deepwalk import DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eeef75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers, input_feat_dim, position_emb_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_feat_dim = input_feat_dim\n",
    "        self.position_emb_dim = position_emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.rsd_encoder = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layers.append(nn.Linear(input_feat_dim + position_emb_dim, hidden_dim))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        for layer in self.layers[:-1]:\n",
    "            inp = F.relu(layer(inp))\n",
    "            inp = self.dropout(inp)\n",
    "        out = self.layers[-1](inp)\n",
    "        return inp, out\n",
    "    \n",
    "    def MLP_RSD(self, mlp_emb):\n",
    "        return F.relu(self.rsd_encoder(mlp_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1d61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='./Cora', name='Cora')\n",
    "\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5816b149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbb0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.data.data.Data"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2742e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1000), tensor(500), tensor(140))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.test_mask), sum(data.val_mask), sum(data.train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "924ad9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(6))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = (list(data.y))\n",
    "min(l), max(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb6db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph_nx = nx.Graph()\n",
    "graph_nx.add_edges_from(data.edge_index.t().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88cd6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  2709\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  100742 lr:  0.000000 avg.loss:  3.680137 ETA:   0h 0m 0s\n",
      "/Users/plslokeshreddy/Documents/projects/lwg/CS768-Learning-with-Graphs/deepwalk/deepwalk.py:63: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  return torch.tensor(embeddings)\n"
     ]
    }
   ],
   "source": [
    "deepwalk_model = DeepWalk(graph_nx, walk_length=80, walks_per_vertex=10)\n",
    "deepwalk_model.train()\n",
    "position_embeddings = deepwalk_model.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2d89a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "487f4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(num_layers=3,\n",
    "          input_feat_dim=data.x.shape[1],\n",
    "          position_emb_dim=position_embeddings.shape[1],\n",
    "          hidden_dim=64,\n",
    "          output_dim=dataset.num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mlp = mlp.to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "71ebb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_emb = torch.load('teacher_outputs/embeddings.pt')\n",
    "teacher_out = torch.load('teacher_outputs/label_scores.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "41480021",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss_fn = nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8a29c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_delta(model, feats, labels, train_mask):\n",
    "    iters = 5\n",
    "    eps = 0.05\n",
    "    alpha = eps / 4\n",
    "\n",
    "    # init\n",
    "    delta = torch.rand(feats.shape) * eps * 2 - eps\n",
    "    delta = delta.to(feats.device)\n",
    "    delta = torch.nn.Parameter(delta)\n",
    "\n",
    "    for i in range(iters):\n",
    "        p_feats = feats + delta\n",
    "\n",
    "        _, logits = model(p_feats)\n",
    "        # out = logits.log_softmax(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        loss.backward()\n",
    "\n",
    "        # delta update\n",
    "        delta.data = delta.data + alpha * delta.grad.sign()\n",
    "        delta.grad = None\n",
    "        delta.data = torch.clamp(delta.data, min=-eps, max=eps)\n",
    "\n",
    "    output = delta.detach()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "47d33f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Loss: 53.3479, Train Acc: 0.3929, Val Acc: 0.3440, Test Acc: 0.3470\n",
      "Epoch 010, Loss: 48.9328, Train Acc: 0.4429, Val Acc: 0.1360, Test Acc: 0.1530\n",
      "Epoch 020, Loss: 29.8446, Train Acc: 0.7714, Val Acc: 0.5840, Test Acc: 0.5850\n",
      "Epoch 030, Loss: 23.8138, Train Acc: 0.9286, Val Acc: 0.6480, Test Acc: 0.6810\n",
      "Epoch 040, Loss: 19.9465, Train Acc: 0.9786, Val Acc: 0.7080, Test Acc: 0.7190\n",
      "Epoch 050, Loss: 17.0074, Train Acc: 0.9929, Val Acc: 0.7500, Test Acc: 0.7460\n",
      "Epoch 060, Loss: 15.0716, Train Acc: 1.0000, Val Acc: 0.7600, Test Acc: 0.7630\n",
      "Epoch 070, Loss: 13.4363, Train Acc: 1.0000, Val Acc: 0.7640, Test Acc: 0.7840\n",
      "Epoch 080, Loss: 12.2490, Train Acc: 1.0000, Val Acc: 0.7780, Test Acc: 0.7990\n",
      "Epoch 090, Loss: 11.4245, Train Acc: 1.0000, Val Acc: 0.7680, Test Acc: 0.7970\n",
      "Epoch 100, Loss: 10.6364, Train Acc: 1.0000, Val Acc: 0.7760, Test Acc: 0.7890\n",
      "Epoch 110, Loss: 9.7586, Train Acc: 1.0000, Val Acc: 0.7820, Test Acc: 0.7910\n",
      "Epoch 120, Loss: 9.7697, Train Acc: 1.0000, Val Acc: 0.7760, Test Acc: 0.7980\n",
      "Epoch 130, Loss: 9.0348, Train Acc: 1.0000, Val Acc: 0.7740, Test Acc: 0.7960\n",
      "Epoch 140, Loss: 8.6151, Train Acc: 1.0000, Val Acc: 0.7780, Test Acc: 0.7970\n",
      "Epoch 150, Loss: 8.3827, Train Acc: 1.0000, Val Acc: 0.7740, Test Acc: 0.7980\n",
      "Epoch 160, Loss: 8.4012, Train Acc: 1.0000, Val Acc: 0.7780, Test Acc: 0.7970\n",
      "Epoch 170, Loss: 7.9709, Train Acc: 1.0000, Val Acc: 0.7760, Test Acc: 0.7940\n",
      "Epoch 180, Loss: 8.0565, Train Acc: 1.0000, Val Acc: 0.7760, Test Acc: 0.7970\n",
      "Epoch 190, Loss: 7.4133, Train Acc: 1.0000, Val Acc: 0.7740, Test Acc: 0.7940\n",
      "Epoch 200, Loss: 7.2410, Train Acc: 1.0000, Val Acc: 0.7740, Test Acc: 0.7970\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    mlp.train()\n",
    "    optimizer.zero_grad()\n",
    "    inp = torch.cat([data.x, position_embeddings.to(device)], dim=-1)  \n",
    "    mlp_emb, mlp_out = mlp(inp)\n",
    "\n",
    "    # GROUND TRUTH Cross Entropy Loss\n",
    "    GT_loss = F.cross_entropy(mlp_out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    # SOFT LABELS KL Divergence Loss\n",
    "    temp = 1\n",
    "    SL_Loss = kl_loss_fn(F.log_softmax(mlp_out/temp, dim=1), \n",
    "                         F.softmax(teacher_out/temp, dim=1)) * (temp**2)\n",
    "    \n",
    "    # Representational Similarity Distillation Loss\n",
    "    teacher_mat = teacher_emb @ teacher_emb.t()\n",
    "    encoded_mlp_meb = mlp.MLP_RSD(mlp_emb)\n",
    "    mlp_mat = encoded_mlp_meb @ encoded_mlp_meb.t()\n",
    "    RSD_Loss = torch.mean((mlp_mat - teacher_mat) ** 2)\n",
    "\n",
    "    # Adversarial Feature Augmentation Loss\n",
    "    inp = torch.cat([data.x, position_embeddings.to(device)], dim=-1)  \n",
    "    delta = pgd_delta(mlp, inp, data.y, data.train_mask)\n",
    "    _, mlp_out = mlp(inp + delta)\n",
    "    ADV_loss = F.cross_entropy(mlp_out[data.train_mask], data.y[data.train_mask]) + F.cross_entropy(mlp_out, F.softmax(teacher_out, dim=1))\n",
    "    \n",
    "    loss = 1.0*GT_loss + 0.5*SL_Loss + 0.1*RSD_Loss + 0.3*ADV_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    mlp.eval()\n",
    "    inp = torch.cat([data.x, position_embeddings.to(device)], dim=-1)  \n",
    "    _, out = mlp(inp)\n",
    "    pred = out.argmax(dim=1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask] == data.y[mask]\n",
    "        acc = int(correct.sum()) / int(mask.sum())\n",
    "        accs.append(acc)\n",
    "    return accs  # train_acc, val_acc, test_acc\n",
    "\n",
    "# Training for 200 epochs\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5aec4800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 4,  ..., 3, 3, 3])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd300b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
